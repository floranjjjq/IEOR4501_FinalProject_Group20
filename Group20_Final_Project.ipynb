{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import sqlalchemy as db\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import text\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from typing import List, Optional, Tuple, Dict, Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622506e",
   "metadata": {},
   "source": [
    "### 1.1 Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef45a8",
   "metadata": {},
   "source": [
    "#### 1.1.1 Define the function ***filter_links_by_date*** to scrap the data only from January 2020 to August 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links_by_date(\n",
    "    links: List[str], \n",
    "    start_year: int, \n",
    "    start_month: int, \n",
    "    end_year: int, \n",
    "    end_month: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filters a list of links to include only those within the specified date range.\n",
    "\n",
    "    Args:\n",
    "        links (list): List of URLs to filter.\n",
    "        start_year (int): Starting year of the range (inclusive).\n",
    "        start_month (int): Starting month of the range (inclusive).\n",
    "        end_year (int): Ending year of the range (inclusive).\n",
    "        end_month (int): Ending month of the range (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list of URLs.\n",
    "    \"\"\"\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        # Extract year and month using regex\n",
    "        match = re.search(r\"(\\d{4})-(\\d{2})\", link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            # Check if the year and month fall within the specified range\n",
    "            if (start_year < year < end_year) or \\\n",
    "               (year == start_year and start_month <= month) or \\\n",
    "               (year == end_year and month <= end_month):\n",
    "                filtered_links.append(link)\n",
    "    return filtered_links\n",
    "\n",
    "start_year, start_month = 2020, 1\n",
    "end_year, end_month = 2024, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835fe11",
   "metadata": {},
   "source": [
    "#### 1.1.2 Extract the file links for Yellow Taxi & Uber trip data from the TLC Trip Record Data web pages and then download the filtered dated files to save them to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the NYC Trip Data page\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Define function to download Parquet files\n",
    "def download_parquet_files(links: List[str], save_dir: str):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for link in links:\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        \n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(link, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Saved to {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: HTTP {response.status_code}\")\n",
    "\n",
    "# Scrape the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all anchor tags with href attributes\n",
    "all_links = [a[\"href\"].strip() for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "# Include only Yellow Taxi and HVFHV trip data using re module\n",
    "yellow_taxi_pattern = r\".*yellow_tripdata.*\\.parquet\"\n",
    "hvfhv_pattern = r\".*fhvhv_tripdata.*\\.parquet\"\n",
    "\n",
    "yellow_taxi_links = [link for link in all_links if re.search(yellow_taxi_pattern, link)]\n",
    "filtered_yellow_taxi_links = filter_links_by_date(yellow_taxi_links, start_year, start_month, end_year, end_month)\n",
    "hvfhv_links = [link for link in all_links if re.search(hvfhv_pattern, link)]\n",
    "filtered_hvfhv_links = filter_links_by_date(hvfhv_links, start_year, start_month, end_year, end_month)\n",
    "\n",
    "# Make full URLs if relative paths are present\n",
    "base_url = \"https://www.nyc.gov\"\n",
    "yellow_taxi_links = [\n",
    "    link if link.startswith(\"http\") else base_url + link for link in filtered_yellow_taxi_links\n",
    "]\n",
    "hvfhv_links = [\n",
    "    link if link.startswith(\"http\") else base_url + link for link in hvfhv_links\n",
    "]\n",
    "\n",
    "# Debugging: Print the found links\n",
    "print(\"Yellow Taxi Links:\", filtered_yellow_taxi_links)\n",
    "print(\"HVFHV Links:\", filtered_hvfhv_links)\n",
    "\n",
    "# Download Parquet files\n",
    "print(\"Downloading Yellow Taxi Parquet files...\")\n",
    "download_parquet_files(filtered_yellow_taxi_links, save_dir = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\")\n",
    "\n",
    "print(\"\\nDownloading HVFHV Parquet files...\")\n",
    "download_parquet_files(filtered_hvfhv_links, save_dir = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108987d7",
   "metadata": {},
   "source": [
    "#### 1.1.3 Loading the Taxi Zones shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Taxi Zones shapefile\n",
    "TAXI_ZONES_SHAPEFILE = \"taxi_zones.shp\"\n",
    "\n",
    "# Load the shapefile into a GeoDataFrame\n",
    "taxi_zones_gdf = gpd.read_file(TAXI_ZONES_SHAPEFILE)\n",
    "taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Inspect the GeoDataFrame\n",
    "print(taxi_zones_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706c68c",
   "metadata": {},
   "source": [
    "### 1.2 Related function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fed589",
   "metadata": {},
   "source": [
    "#### 1.2.1 Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> Optional[gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load and process the taxi zones shapefile, extracting longitude and latitude from the geometry column.\n",
    "\n",
    "    Parameters:\n",
    "        shapefile_path (str): Path to the taxi zones shapefile.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Processed GeoDataFrame with longitude and latitude columns added.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Taxi Zones GeoDataFrame\n",
    "        taxi_zones_gdf = gpd.read_file(shapefile)\n",
    "\n",
    "        # Check CRS and reproject to WGS84 (EPSG:4326) if necessary\n",
    "        if taxi_zones_gdf.crs != \"EPSG:4326\":\n",
    "            taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Extract longitude and latitude from the geometry column\n",
    "        taxi_zones_gdf[\"longitude\"] = taxi_zones_gdf.geometry.centroid.x\n",
    "        taxi_zones_gdf[\"latitude\"] = taxi_zones_gdf.geometry.centroid.y\n",
    "\n",
    "        return taxi_zones_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading and processing shapefile: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(\n",
    "    zone_loc_id: int, \n",
    "    loaded_taxi_zones: gpd.GeoDataFrame\n",
    ") -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Look up the coordinates (longitude, latitude) of a taxi zone given its Location ID.\n",
    "\n",
    "    Parameters:\n",
    "    - zone_loc_id (int): The Location ID of the taxi zone.\n",
    "    - loaded_taxi_zones (gpd.GeoDataFrame): The GeoDataFrame containing the taxi zones.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple (longitude, latitude) representing the coordinates of the taxi zone.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter the GeoDataFrame for the given Location ID\n",
    "        zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "\n",
    "        # Check if the zone is valid\n",
    "        if zone.empty:\n",
    "            raise ValueError(f\"Location ID {zone_loc_id} not found in the Taxi Zones dataset.\")\n",
    "\n",
    "        # Use the centroid of the zone polygon for coordinates\n",
    "        longitude = zone.geometry.centroid.x.values[0]\n",
    "        latitude = zone.geometry.centroid.y.values[0]\n",
    "\n",
    "        # Return as a tuple\n",
    "        return longitude, latitude\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error looking up coordinates for Location ID {zone_loc_id}: {e}\")\n",
    "        return None, None  # Return None for both longitude and latitude if there's an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b1522",
   "metadata": {},
   "source": [
    "#### 1.2.2 Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_taxi_sample_size(population: Union[int, float]) -> int:\n",
    "    '''\n",
    "    Calculate the sample size needed for a given population\n",
    "\n",
    "    Parameters:\n",
    "    - population: total population size\n",
    "\n",
    "    Returns:\n",
    "    - the sample size needed\n",
    "    '''\n",
    "    # set confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    z_score = 1.96 # for 95% confidence level\n",
    "    proportion = 0.5 # proportion of the population that has the attribute of interest (use 0.5 for max variance)\n",
    "\n",
    "    # implement Cochran's formula\n",
    "    sample_size = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "\n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f623cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fhvhv_sample_size(population: Union[int, float]) -> int:\n",
    "    '''\n",
    "    Calculate the sample size needed for a given population\n",
    "\n",
    "    Parameters:\n",
    "    - population: total population size\n",
    "\n",
    "    Returns:\n",
    "    - the sample size needed\n",
    "    '''\n",
    "    # set confidence_level = 0.99\n",
    "    margin_of_error = 0.05\n",
    "    z_score = 2.58 # for 95% confidence level\n",
    "    proportion = 0.5 # proportion of the population that has the attribute of interest (use 0.5 for max variance)\n",
    "\n",
    "    # implement Cochran's formula\n",
    "    sample_size = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "\n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bafdd",
   "metadata": {},
   "source": [
    "### 1.2.3 Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page: str) -> List[str]:\n",
    "    '''\n",
    "    Get all the URLs from the TLC page\n",
    "    \n",
    "    Parameters:\n",
    "    - taxi_page: the URL of the TLC page\n",
    "    \n",
    "    Returns:\n",
    "    - a list of all URLs on the page\n",
    "    '''\n",
    "    \n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    all_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c441edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls: List[str]) -> Dict[str, List[str]]:\n",
    "    '''\n",
    "    Filter the URLs to get only the Parquet files\n",
    "    \n",
    "    Parameters:\n",
    "    - all_urls: a list of all URLs\n",
    "    \n",
    "    Returns:\n",
    "    - a list of URLs that contain the word \"parquet\"\n",
    "    '''\n",
    "    \n",
    "    # Define patterns for Yellow Taxi and HVFHV Parquet files\n",
    "    yellow_taxi_pattern = r\".*yellow_tripdata.*\\.parquet\"\n",
    "    hvfhv_pattern = r\".*fhvhv_tripdata.*\\.parquet\"\n",
    "\n",
    "    # Filter URLs for each dataset\n",
    "    yellow_taxi_urls = [url for url in all_urls if re.search(yellow_taxi_pattern, url)]\n",
    "    hvfhv_urls = [url for url in all_urls if re.search(hvfhv_pattern, url)]\n",
    "\n",
    "    return {\n",
    "        \"yellow_taxi\": yellow_taxi_urls,\n",
    "        \"hvfhv\": hvfhv_urls,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9fe1c",
   "metadata": {},
   "source": [
    "### 1.3 Process Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e6db3",
   "metadata": {},
   "source": [
    "#### 1.3.1 Generate random samples of taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0449bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the taxi monthly datasets\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    if \"yellow\" in file_name and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # Read the dataset\n",
    "        data = pd.read_parquet(file_path)\n",
    "        population_size = len(data)  # Get total rows in the dataset\n",
    "\n",
    "        # Calculate sample size dynamically\n",
    "        sample_size = calculate_taxi_sample_size(population_size)\n",
    "\n",
    "        # Create a random sample\n",
    "        sampled_data = data.sample(n=sample_size)\n",
    "\n",
    "        # Save the sampled data to a new file\n",
    "        output_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_path)\n",
    "\n",
    "        # Print information for debugging\n",
    "        print(f\"Processed {file_name}: Population = {population_size}, Sample = {sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39480170",
   "metadata": {},
   "source": [
    "#### 1.3.2 Cleaning and processing taxi sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC bounding box coordinates (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LAT_MIN, LON_MIN = NEW_YORK_BOX_COORDS[0]\n",
    "LAT_MAX, LON_MAX = NEW_YORK_BOX_COORDS[1]\n",
    "CRS = 4326\n",
    "\n",
    "def get_and_clean_taxi_data(\n",
    "    file_path: str, \n",
    "    save_dir: str, \n",
    "    taxi_zones_gdf: gpd.GeoDataFrame\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean a single month's taxi data from a local Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        save_dir (str): Directory where cleaned files will be saved.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame of taxi zones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame for the given month.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the Parquet file\n",
    "        print(f\"Loading file: {file_name}...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "        # Step 3: Look up and add coordinates for pulocationid and dolocationid\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            valid_location_ids = taxi_zones_gdf[\"LocationID\"]\n",
    "            initial_rows = len(df)\n",
    "            df = df[df[\"pulocationid\"].isin(valid_location_ids) & df[\"dolocationid\"].isin(valid_location_ids)]\n",
    "            dropped_rows = initial_rows - len(df)\n",
    "            print(f\"Dropped {dropped_rows} rows with invalid Location IDs.\")\n",
    "        else:\n",
    "            print(f\"Columns 'pulocationid' or 'dolocationid' not found in {file_name}. Skipping location ID filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 5: Drop rows with the same pickup and dropoff location\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        filtered_rows = initial_rows - len(df)\n",
    "        print(f\"Filtered out {filtered_rows} rows with the same pickup and dropoff location.\")\n",
    "\n",
    "        # Step 6: Look up and add coordinates for pulocationid and dolocationid\n",
    "        print(\"Looking up coordinates for location IDs...\")\n",
    "        df[\"pickup_coords\"] = df[\"pulocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "        df[\"dropoff_coords\"] = df[\"dolocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "\n",
    "        # Split coordinates into latitude and longitude columns\n",
    "        df[[\"pickup_longitude\", \"pickup_latitude\"]] = pd.DataFrame(\n",
    "            df[\"pickup_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "        df[[\"dropoff_longitude\", \"dropoff_latitude\"]] = pd.DataFrame(\n",
    "            df[\"dropoff_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "\n",
    "        # Drop temporary coordinate columns\n",
    "        df.drop([\"pickup_coords\", \"dropoff_coords\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        # Step 7: Remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count',\n",
    "            'trip_distance', 'pulocationid', 'dolocationid', 'fare_amount', 'extra',\n",
    "            'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', \n",
    "            'congestion_surcharge', 'airport_fee', 'pickup_latitude', 'pickup_longitude',\n",
    "            'dropoff_latitude', 'dropoff_longitude'\n",
    "        ]\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Step 8: Normalize datetime columns\n",
    "        for time_col in ['tpep_pickup_datetime', 'tpep_dropoff_datetime']:\n",
    "            if time_col in df.columns:\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "                \n",
    "        if \"airport_fee\" in df.columns:\n",
    "            df[\"airport_fee\"] = df[\"airport_fee\"].fillna(0) \n",
    "            \n",
    "        # Step 9: Filter trips within NYC bounding box\n",
    "        if {\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"}.issubset(df.columns):\n",
    "            df = df[\n",
    "                (df['pickup_latitude'] >= LAT_MIN) & (df['pickup_latitude'] <= LAT_MAX) &\n",
    "                (df['pickup_longitude'] >= LON_MIN) & (df['pickup_longitude'] <= LON_MAX) &\n",
    "                (df['dropoff_latitude'] >= LAT_MIN) & (df['dropoff_latitude'] <= LAT_MAX) &\n",
    "                (df['dropoff_longitude'] >= LON_MIN) & (df['dropoff_longitude'] <= LON_MAX)\n",
    "            ]\n",
    "\n",
    "        # Step 10: Remove rides with the same pickup and dropoff locations or zero distance\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        if \"trip_distance\" in df.columns:\n",
    "            df = df[df[\"trip_distance\"] > 0]\n",
    "\n",
    "        # Step 11: Rename columns for clarity\n",
    "        df.rename(columns={\n",
    "            \"tpep_pickup_datetime\": \"pickup_time\",\n",
    "            \"tpep_dropoff_datetime\": \"dropoff_time\",\n",
    "            \"pulocationid\": \"pickup_location_id\",\n",
    "            \"dolocationid\": \"dropoff_location_id\",\n",
    "            \"extra\": \"miscellaneous_extra_charges\"\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        output_path = os.path.join(save_dir, f\"cleaned_{file_name}\")\n",
    "        df.to_parquet(output_path)\n",
    "        print(f\"Cleaned {len(df)} rows and saved to {output_path}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main Script\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load the Taxi Zones GeoDataFrame\n",
    "TAXI_ZONES_SHAPEFILE = os.path.join(directory, \"taxi_zones.shp\")\n",
    "taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "# Iterate over each Parquet file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    # Process only files that contain \"yellow\" and end with \".parquet\"\n",
    "    if \"sampled_yellow\" in file_name.lower() and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        get_and_clean_taxi_data(file_path, output_directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1428a",
   "metadata": {},
   "source": [
    "#### 1.3.3 Merge the cleaned .parquet files into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data(cleaned_files_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine cleaned taxi data from local cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all months.\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "\n",
    "    # Iterate through all cleaned Parquet files in the directory\n",
    "    for file_name in os.listdir(cleaned_files_dir):\n",
    "        if file_name.startswith(\"cleaned_\") and file_name.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(cleaned_files_dir, file_name)\n",
    "            print(f\"Loading cleaned file: {file_path}...\")\n",
    "            \n",
    "            # Load the cleaned Parquet file into a DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "            all_taxi_dataframes.append(df)\n",
    "\n",
    "    # Combine all cleaned data into a single DataFrame\n",
    "    if all_taxi_dataframes:\n",
    "        taxi_data = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "        print(f\"Combined {len(taxi_data)} rows from all cleaned files.\")\n",
    "    else:\n",
    "        taxi_data = pd.DataFrame()\n",
    "        print(\"No cleaned files found to process.\")\n",
    "\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_taxi_data(cleaned_files_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all cleaned Yellow Taxi and HVFHV data from cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all cleaned taxi data.\n",
    "    \"\"\"\n",
    "    # Process all cleaned Parquet files in the directory\n",
    "    print(\"Combining all cleaned taxi data...\")\n",
    "    all_data = get_taxi_data(cleaned_files_dir=cleaned_files_dir)\n",
    "\n",
    "    print(f\"Final combined data contains {len(all_data)} rows.\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_FILES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Load and combine all cleaned data into a single DataFrame\n",
    "taxi_data = get_all_taxi_data(cleaned_files_dir=CLEANED_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### 1.4 Processing Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648f47b",
   "metadata": {},
   "source": [
    "#### 1.4.1 Generate random samples of uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1856e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the taxi monthly datasets\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    if \"fhvhv\" in file_name and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # Read the dataset\n",
    "        data = pd.read_parquet(file_path)\n",
    "        population_size = len(data)  # Get total rows in the dataset\n",
    "\n",
    "        # Calculate sample size dynamically\n",
    "        sample_size = calculate_fhvhv_sample_size(population_size)\n",
    "\n",
    "        # Create a random sample\n",
    "        sampled_data = data.sample(n=sample_size)\n",
    "\n",
    "        # Save the sampled data to a new file\n",
    "        output_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_path)\n",
    "\n",
    "        # Print information for debugging\n",
    "        print(f\"Processed {file_name}: Population = {population_size}, Sample = {sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee46796",
   "metadata": {},
   "source": [
    "#### 1.4.2 Cleaning and processing uber sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(\n",
    "    file_path: str, \n",
    "    save_dir: str, \n",
    "    taxi_zones_gdf: gpd.GeoDataFrame\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean a single month's Uber data from a local Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        save_dir (str): Directory where cleaned files will be saved.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame of taxi zones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame for the given month.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the Parquet file\n",
    "        print(f\"Loading file: {file_name}...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "        # Step 3: Filter out non-Uber rides\n",
    "        if \"hvfhs_license_num\" in df.columns:\n",
    "            df = df[df[\"hvfhs_license_num\"].str.contains(\"HV0003\", na=False)]\n",
    "            print(f\"Filtered to Uber rides only: {len(df)} rows remaining.\")\n",
    "        else:\n",
    "            print(f\"'hvfhs_license_num' column not found in {file_name}. Skipping Uber filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 4: Drop rows with invalid Location IDs\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            valid_location_ids = set(taxi_zones_gdf[\"LocationID\"])\n",
    "            initial_rows = len(df)\n",
    "            df = df[df[\"pulocationid\"].isin(valid_location_ids) & df[\"dolocationid\"].isin(valid_location_ids)]\n",
    "            dropped_rows = initial_rows - len(df)\n",
    "            print(f\"Dropped {dropped_rows} rows with invalid Location IDs.\")\n",
    "        else:\n",
    "            print(f\"Columns 'pulocationid' or 'dolocationid' not found in {file_name}. Skipping location ID filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 5: Drop rows with the same pickup and dropoff location\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        filtered_rows = initial_rows - len(df)\n",
    "        print(f\"Filtered out {filtered_rows} rows with the same pickup and dropoff location.\")\n",
    "\n",
    "        # Step 6: Look up and add coordinates for pulocationid and dolocationid\n",
    "        print(\"Looking up coordinates for location IDs...\")\n",
    "        df[\"pickup_coords\"] = df[\"pulocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "        df[\"dropoff_coords\"] = df[\"dolocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "\n",
    "        # Split coordinates into latitude and longitude columns\n",
    "        df[[\"pickup_longitude\", \"pickup_latitude\"]] = pd.DataFrame(\n",
    "            df[\"pickup_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "        df[[\"dropoff_longitude\", \"dropoff_latitude\"]] = pd.DataFrame(\n",
    "            df[\"dropoff_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "\n",
    "        # Drop temporary coordinate columns\n",
    "        df.drop([\"pickup_coords\", \"dropoff_coords\"], axis=1, inplace=True)\n",
    "\n",
    "        # Step 7: Remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid',\n",
    "            'trip_miles', 'base_passenger_fare', 'tolls', 'sales_tax', \n",
    "            'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
    "            'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'\n",
    "        ]\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Step 8: Normalize datetime columns\n",
    "        for time_col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            if time_col in df.columns:\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "        if 'trip_time' in df.columns:\n",
    "            df['trip_time'] = pd.to_timedelta(df['trip_time'], errors='coerce')\n",
    "\n",
    "        if \"airport_fee\" in df.columns:\n",
    "            df[\"airport_fee\"] = df[\"airport_fee\"].fillna(0) \n",
    "\n",
    "        # Step 9: Filter trips within NYC bounding box\n",
    "        if {\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"}.issubset(df.columns):\n",
    "            df = df[\n",
    "                (df['pickup_latitude'] >= LAT_MIN) & (df['pickup_latitude'] <= LAT_MAX) &\n",
    "                (df['pickup_longitude'] >= LON_MIN) & (df['pickup_longitude'] <= LON_MAX) &\n",
    "                (df['dropoff_latitude'] >= LAT_MIN) & (df['dropoff_latitude'] <= LAT_MAX) &\n",
    "                (df['dropoff_longitude'] >= LON_MIN) & (df['dropoff_longitude'] <= LON_MAX)\n",
    "            ]\n",
    "\n",
    "        # Step 10: Rename columns for clarity\n",
    "        df.rename(columns={\n",
    "            \"pickup_datetime\": \"pickup_time\",\n",
    "            \"dropoff_datetime\": \"dropoff_time\",\n",
    "            \"pulocationid\": \"pickup_location_id\",\n",
    "            \"dolocationid\": \"dropoff_location_id\",\n",
    "            \"trip_miles\": \"trip_distance\",\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        output_path = os.path.join(save_dir, f\"cleaned_{file_name}\")\n",
    "        df.to_parquet(output_path)\n",
    "        print(f\"Cleaned {len(df)} rows and saved to {output_path}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Script\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load the Taxi Zones GeoDataFrame\n",
    "TAXI_ZONES_SHAPEFILE = os.path.join(directory, \"taxi_zones.shp\")\n",
    "taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "# Iterate over each Parquet file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    # Process only files that contain \"yellow\" and end with \".parquet\"\n",
    "    if \"sampled_fhvhv\" in file_name.lower() and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        get_and_clean_uber_data(file_path, output_directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e6996",
   "metadata": {},
   "source": [
    "#### 1.4.3 Merge the cleaned .parquet files into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data(\n",
    "    cleaned_files_dir: str, \n",
    "    prefix: str = \"cleaned_sampled_fhvhv\", \n",
    "    file_extension: str = \".parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine cleaned Uber data from local cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "        prefix (str): Prefix to identify Uber files (default is \"cleaned_uber_\").\n",
    "        file_extension (str): File extension to identify Parquet files (default is \".parquet\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all months.\n",
    "    \"\"\"\n",
    "    all_uber_dataframes = []\n",
    "\n",
    "    # Verify that the directory exists\n",
    "    if not os.path.exists(cleaned_files_dir):\n",
    "        print(f\"Directory does not exist: {cleaned_files_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in os.listdir(cleaned_files_dir):\n",
    "        # Filter files by prefix and file extension\n",
    "        if file_name.startswith(prefix) and file_name.endswith(file_extension):\n",
    "            file_path = os.path.join(cleaned_files_dir, file_name)\n",
    "            print(f\"Loading cleaned Uber file: {file_path}...\")\n",
    "            \n",
    "            # Load the cleaned Parquet file into a DataFrame\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                all_uber_dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "    # Combine all cleaned data into a single DataFrame\n",
    "    if all_uber_dataframes:\n",
    "        uber_data = pd.concat(all_uber_dataframes, ignore_index=True)\n",
    "        print(f\"Combined {len(uber_data)} rows from all cleaned Uber files.\")\n",
    "    else:\n",
    "        uber_data = pd.DataFrame()\n",
    "        print(\"No cleaned Uber files found to process.\")\n",
    "\n",
    "    return uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_uber_data(\n",
    "    cleaned_files_dir: str, \n",
    "    prefix: str = \"cleaned_sampled_fhvhv\", \n",
    "    file_extension: str = \".parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all cleaned Uber data from cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "        prefix (str): Prefix to identify Uber files (default is \"cleaned_uber_\").\n",
    "        file_extension (str): File extension to identify Parquet files (default is \".parquet\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all cleaned Uber data.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(cleaned_files_dir):\n",
    "        print(f\"Directory does not exist: {cleaned_files_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Combine all cleaned Uber data\n",
    "    print(f\"Combining all cleaned Uber data from directory: {cleaned_files_dir}...\")\n",
    "    all_uber_data = get_uber_data(cleaned_files_dir=cleaned_files_dir, prefix=prefix, file_extension=file_extension)\n",
    "\n",
    "    # Check if the resulting DataFrame is empty\n",
    "    if all_uber_data.empty:\n",
    "        print(\"No Uber data found or combined. Returning an empty DataFrame.\")\n",
    "    else:\n",
    "        print(f\"Final combined Uber data contains {len(all_uber_data)} rows.\")\n",
    "    \n",
    "    return all_uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script for Uber Data\n",
    "CLEANED_UBER_FILES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Load and combine all cleaned Uber data into a single DataFrame\n",
    "uber_data = get_all_uber_data(cleaned_files_dir=CLEANED_UBER_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95c131",
   "metadata": {},
   "source": [
    "### 1.5 Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b236a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of all CSV files in the given directory that are related to weather data.\n",
    "    Weather-related files are identified by the presence of the keyword 'weather' in the filename.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory to search for files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of weather-related CSV filenames.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"Invalid directory: {directory}\")\n",
    "    \n",
    "    weather_csvs = [\n",
    "        file for file in os.listdir(directory)\n",
    "        if file.endswith('.csv') and 'weather' in file.lower()\n",
    "    ]\n",
    "    \n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb4263",
   "metadata": {},
   "source": [
    "#### 1.5.1 Cleaning and processing weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d84a9f",
   "metadata": {},
   "source": [
    "##### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the weather dataset by:\n",
    "    - Extracting specific columns.\n",
    "    - Normalizing column names by adding underscores between words.\n",
    "    - Converting weather codes to descriptive terms.\n",
    "    - Handling multiple data points within the same hour by selecting the ideal one.\n",
    "    - Filling missing hourly data by interpolating between previous and next data points.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file containing the weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with selected and transformed columns.\n",
    "    \"\"\"\n",
    "    # AU code mapping for HourlyPresentWeatherType\n",
    "    au_code_mapping = {\n",
    "        \"DZ\": \"Drizzle\",\n",
    "        \"RA\": \"Rain\",\n",
    "        \"SN\": \"Snow\",\n",
    "        \"SG\": \"Snow Grains\",\n",
    "        \"IC\": \"Ice Crystals\",\n",
    "        \"PL\": \"Ice Pellets\",\n",
    "        \"GR\": \"Hail\",\n",
    "        \"GS\": \"Small Hail\",\n",
    "        \"UP\": \"Unknown Precipitation\",\n",
    "        \"BR\": \"Mist\",\n",
    "        \"FG\": \"Fog\",\n",
    "        \"FU\": \"Smoke\",\n",
    "        \"VA\": \"Volcanic Ash\",\n",
    "        \"DU\": \"Dust\",\n",
    "        \"SA\": \"Sand\",\n",
    "        \"HZ\": \"Haze\",\n",
    "        \"PY\": \"Spray\",\n",
    "        \"PO\": \"Sand Whirls\",\n",
    "        \"SQ\": \"Squalls\",\n",
    "        \"FC\": \"Funnel Cloud\",\n",
    "        \"SS\": \"Sandstorm\",\n",
    "        \"DS\": \"Duststorm\"\n",
    "    }\n",
    "\n",
    "    # Sky condition mapping for HourlySkyConditions\n",
    "    sky_condition_mapping = {\n",
    "        \"CLR\": \"Clear\",\n",
    "        \"FEW\": \"Few Clouds\",\n",
    "        \"SCT\": \"Scattered Clouds\",\n",
    "        \"BKN\": \"Broken Clouds\",\n",
    "        \"OVC\": \"Overcast\",\n",
    "        \"VV\": \"Obscured Sky\"\n",
    "    }\n",
    "\n",
    "    # Function to interpret HourlyPresentWeatherType using AU codes\n",
    "    def interpret_weather_type(weather_string):\n",
    "        if pd.isnull(weather_string):\n",
    "            return \"Unknown\"\n",
    "        matches = re.findall(r\"([A-Z]{2}):\\d+\", weather_string)\n",
    "        descriptions = [au_code_mapping.get(code, \"Unknown\") for code in matches]\n",
    "        return \", \".join(set(descriptions)) if descriptions else \"Unknown\"\n",
    "\n",
    "    # Function to interpret HourlySkyConditions\n",
    "    def interpret_sky_conditions(sky_string):\n",
    "        if pd.isnull(sky_string):\n",
    "            return \"Unknown\"\n",
    "        pattern = r\"(\\w{3}):(\\d{2})(?:\\s(\\d+))?\"\n",
    "        matches = re.findall(pattern, sky_string)\n",
    "        interpreted_conditions = []\n",
    "        for condition, octa, elevation in matches:\n",
    "            description = sky_condition_mapping.get(condition, \"Unknown\")\n",
    "            detail = f\"{description}, Octas: {int(octa)}\"\n",
    "            if elevation:\n",
    "                detail += f\", Elevation: {int(elevation)} feet\"\n",
    "            interpreted_conditions.append(detail)\n",
    "        return \"; \".join(interpreted_conditions) if interpreted_conditions else \"Unknown\"\n",
    "\n",
    "    # Function to select the ideal row for each hour\n",
    "    def select_ideal_row(group):\n",
    "        if group.empty:\n",
    "            return None\n",
    "        if len(group) == 1:\n",
    "            return group.iloc[0]\n",
    "        middle_of_hour = group.index[0].replace(minute=30, second=0, microsecond=0)\n",
    "        time_diffs = abs((group.index - middle_of_hour).total_seconds())\n",
    "        return group.iloc[time_diffs.argmin()]\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    # Normalize column names (add underscores between words and make lowercase)\n",
    "    df.columns = df.columns.str.replace(r\"([a-z])([A-Z])\", r\"\\1_\\2\", regex=True).str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    column_renaming = {\n",
    "        \"date\": \"hourly_time\",\n",
    "        \"hourly_dry_bulb_temperature\": \"hourly_temperature\"\n",
    "    }\n",
    "    df.rename(columns=column_renaming, inplace=True)\n",
    "\n",
    "    # Select only the required columns\n",
    "    columns_to_extract = [\n",
    "        \"hourly_time\", \"hourly_temperature\", \"hourly_present_weather_type\", \n",
    "        \"hourly_sky_conditions\", \"hourly_visibility\", \"hourly_precipitation\", \"hourly_wind_speed\"\n",
    "    ]\n",
    "    df = df[[col for col in columns_to_extract if col in df.columns]]\n",
    "\n",
    "    # Convert hourly_time to datetime\n",
    "    df[\"hourly_time\"] = pd.to_datetime(df[\"hourly_time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"hourly_time\"])  # Drop rows with invalid dates\n",
    "    df = df.sort_values(\"hourly_time\")  # Sort by hourly_time\n",
    "    df.set_index(\"hourly_time\", inplace=True)  # Set hourly_time as the index\n",
    "\n",
    "    # Group by hourly and select the ideal row\n",
    "    df_hourly = df.groupby(pd.Grouper(freq='H')).apply(select_ideal_row)\n",
    "    df_hourly = df_hourly.dropna()  # Drop any None rows from empty groups\n",
    "\n",
    "    # Reindex to include all hourly intervals and interpolate missing data\n",
    "    all_hours = pd.date_range(start=df_hourly.index.min(), end=df_hourly.index.max(), freq='H')\n",
    "    df_hourly = df_hourly.reindex(all_hours)\n",
    "    df_hourly.index.name = \"hourly_time\"\n",
    "\n",
    "    # Replace NaN in hourly_precipitation with 0, and handle 'T' values\n",
    "    if \"hourly_precipitation\" in df_hourly.columns:\n",
    "    # Replace known non-numeric values\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].replace(\"T\", \"0.01\")\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].replace(\"\", \"0\")\n",
    "    # Remove any leftover non-numeric characters (e.g., \"s\")\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "    # Convert to float, coercing any remaining errors to NaN\n",
    "        df_hourly[\"hourly_precipitation\"] = pd.to_numeric(df_hourly[\"hourly_precipitation\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "    # Apply forward-fill for hourly_wind_speed -- assume wind doesn't change much within an hour\n",
    "    if \"hourly_wind_speed\" in df_hourly.columns:\n",
    "        df_hourly[\"hourly_wind_speed\"] = df_hourly[\"hourly_wind_speed\"].fillna(method='ffill')\n",
    "\n",
    "    # Convert numerical columns to numeric, coercing errors to NaN\n",
    "    numerical_cols = [\"hourly_temperature\", \"hourly_visibility\"]\n",
    "    for col in numerical_cols:\n",
    "        df_hourly[col] = pd.to_numeric(df_hourly[col], errors=\"coerce\")\n",
    "\n",
    "    # Interpolate numerical columns\n",
    "    df_hourly[numerical_cols] = df_hourly[numerical_cols].interpolate(method='time')\n",
    "\n",
    "    # Fill categorical columns with previous value or next value if previous is missing\n",
    "    categorical_cols = [\"hourly_present_weather_type\", \"hourly_sky_conditions\"]\n",
    "    df_hourly[categorical_cols] = df_hourly[categorical_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Reset index to bring hourly_time back as a column\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    # Convert hourly_present_weather_type to descriptive terms\n",
    "    df_hourly[\"hourly_present_weather_type\"] = df_hourly[\"hourly_present_weather_type\"].apply(interpret_weather_type)\n",
    "\n",
    "    # Convert hourly_sky_conditions to descriptive terms\n",
    "    df_hourly[\"hourly_sky_conditions\"] = df_hourly[\"hourly_sky_conditions\"].apply(interpret_sky_conditions)\n",
    "\n",
    "    return df_hourly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e112f",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the weather dataset for daily data by:\n",
    "    - Extracting specific columns.\n",
    "    - Interpreting the DailyWeather column using AU codes.\n",
    "    - Converting Sunrise and Sunset columns to time format.\n",
    "    - Replacing None in Sunrise and Sunset columns with the previous day's value.\n",
    "    - Removing rows with NaN values.\n",
    "    - Normalizing column names by adding underscores between words.\n",
    "    - Replacing 'dry_bulb_temperature' with 'temperature' in column names.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file containing the weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with selected and transformed columns.\n",
    "    \"\"\"\n",
    "    # AU code mapping for DailyWeather\n",
    "    au_code_mapping = {\n",
    "        \"DZ\": \"Drizzle\",\n",
    "        \"RA\": \"Rain\",\n",
    "        \"SN\": \"Snow\",\n",
    "        \"SG\": \"Snow Grains\",\n",
    "        \"IC\": \"Ice Crystals\",\n",
    "        \"PL\": \"Ice Pellets\",\n",
    "        \"GR\": \"Hail\",\n",
    "        \"GS\": \"Small Hail\",\n",
    "        \"UP\": \"Unknown Precipitation\",\n",
    "        \"BR\": \"Mist\",\n",
    "        \"FG\": \"Fog\",\n",
    "        \"FU\": \"Smoke\",\n",
    "        \"VA\": \"Volcanic Ash\",\n",
    "        \"DU\": \"Dust\",\n",
    "        \"SA\": \"Sand\",\n",
    "        \"HZ\": \"Haze\",\n",
    "        \"PY\": \"Spray\",\n",
    "        \"PO\": \"Sand Whirls\",\n",
    "        \"SQ\": \"Squalls\",\n",
    "        \"FC\": \"Funnel Cloud\",\n",
    "        \"SS\": \"Sandstorm\",\n",
    "        \"DS\": \"Duststorm\"\n",
    "    }\n",
    "\n",
    "    # Function to interpret DailyWeather using AU codes\n",
    "    def interpret_daily_weather(weather_string):\n",
    "        if pd.isnull(weather_string):\n",
    "            return \"Unknown\"\n",
    "        matches = re.findall(r\"([A-Z]{2})[:\\d]*\", weather_string)\n",
    "        descriptions = [au_code_mapping.get(code, \"Unknown\") for code in matches]\n",
    "        return \", \".join(set(descriptions)) if descriptions else \"Unknown\"\n",
    "\n",
    "    # Function to convert time in integer format (e.g., 720 -> 7:20 am)\n",
    "    def convert_to_time_format(time_int):\n",
    "        if pd.isnull(time_int):\n",
    "            return None\n",
    "        try:\n",
    "            time_str = f\"{int(time_int):04d}\"  # Ensure it's always 4 digits (e.g., 720 -> \"0720\")\n",
    "            return datetime.strptime(time_str, \"%H%M\").time()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    # Select only the required columns\n",
    "    columns_to_extract = [\n",
    "        \"DATE\", \"Sunrise\", \"Sunset\", \"DailyAverageDryBulbTemperature\", \n",
    "        \"DailyAverageWindSpeed\", \"DailyMaximumDryBulbTemperature\", \n",
    "        \"DailyMinimumDryBulbTemperature\", \"DailyWeather\", \"DailySnowDepth\"\n",
    "    ]\n",
    "    df = df[[col for col in columns_to_extract if col in df.columns]]\n",
    "\n",
    "    # Convert DATE to datetime\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DATE\"])  # Drop rows with invalid dates\n",
    "    df = df.sort_values(\"DATE\")  # Sort by date\n",
    "\n",
    "    # Convert Sunrise and Sunset columns to time format\n",
    "    if \"Sunrise\" in df.columns:\n",
    "        df[\"Sunrise\"] = df[\"Sunrise\"].apply(convert_to_time_format)\n",
    "    if \"Sunset\" in df.columns:\n",
    "        df[\"Sunset\"] = df[\"Sunset\"].apply(convert_to_time_format)\n",
    "\n",
    "    # Replace None in Sunrise and Sunset columns with the previous day's value\n",
    "    df[\"Sunrise\"] = df[\"Sunrise\"].fillna(method='ffill')\n",
    "    df[\"Sunset\"] = df[\"Sunset\"].fillna(method='ffill')\n",
    "\n",
    "    # Interpret DailyWeather column\n",
    "    if \"DailyWeather\" in df.columns:\n",
    "        df[\"DailyWeather\"] = df[\"DailyWeather\"].apply(interpret_daily_weather)\n",
    "\n",
    "    # Convert DailySnowDepth column to numeric and handle 'T' values\n",
    "    if \"DailySnowDepth\" in df.columns:\n",
    "        df[\"DailySnowDepth\"] = df[\"DailySnowDepth\"].replace(\"T\", 0.01).astype(float)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Rename columns to use underscores between words\n",
    "    df.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower() for col in df.columns]\n",
    "\n",
    "    # Replace 'dry_bulb_temperature' with 'temperature' in column names\n",
    "    df.columns = [col.replace(\"dry_bulb_temperature\", \"temperature\") for col in df.columns]\n",
    "\n",
    "    df = df.rename(columns={\"d_a_t_e\": \"date\"})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_CSV_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
