{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import sqlalchemy as db\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import text\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from typing import List, Optional, Tuple, Dict, Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622506e",
   "metadata": {},
   "source": [
    "### 1.1 Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef45a8",
   "metadata": {},
   "source": [
    "#### 1.1.1 Define the function ***filter_links_by_date*** to scrap the data only from January 2020 to August 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links_by_date(\n",
    "    links: List[str], \n",
    "    start_year: int, \n",
    "    start_month: int, \n",
    "    end_year: int, \n",
    "    end_month: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filters a list of links to include only those within the specified date range.\n",
    "\n",
    "    Args:\n",
    "        links (list): List of URLs to filter.\n",
    "        start_year (int): Starting year of the range (inclusive).\n",
    "        start_month (int): Starting month of the range (inclusive).\n",
    "        end_year (int): Ending year of the range (inclusive).\n",
    "        end_month (int): Ending month of the range (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list of URLs.\n",
    "    \"\"\"\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        # Extract year and month using regex\n",
    "        match = re.search(r\"(\\d{4})-(\\d{2})\", link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            # Check if the year and month fall within the specified range\n",
    "            if (start_year < year < end_year) or \\\n",
    "               (year == start_year and start_month <= month) or \\\n",
    "               (year == end_year and month <= end_month):\n",
    "                filtered_links.append(link)\n",
    "    return filtered_links\n",
    "\n",
    "start_year, start_month = 2020, 1\n",
    "end_year, end_month = 2024, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835fe11",
   "metadata": {},
   "source": [
    "#### 1.1.2 Extract the file links for Yellow Taxi & Uber trip data from the TLC Trip Record Data web pages and then download the filtered dated files to save them to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the NYC Trip Data page\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Define function to download Parquet files\n",
    "def download_parquet_files(links: List[str], save_dir: str):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for link in links:\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        \n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(link, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Saved to {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: HTTP {response.status_code}\")\n",
    "\n",
    "# Scrape the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all anchor tags with href attributes\n",
    "all_links = [a[\"href\"].strip() for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "# Include only Yellow Taxi and HVFHV trip data using re module\n",
    "yellow_taxi_pattern = r\".*yellow_tripdata.*\\.parquet\"\n",
    "hvfhv_pattern = r\".*fhvhv_tripdata.*\\.parquet\"\n",
    "\n",
    "yellow_taxi_links = [link for link in all_links if re.search(yellow_taxi_pattern, link)]\n",
    "filtered_yellow_taxi_links = filter_links_by_date(yellow_taxi_links, start_year, start_month, end_year, end_month)\n",
    "hvfhv_links = [link for link in all_links if re.search(hvfhv_pattern, link)]\n",
    "filtered_hvfhv_links = filter_links_by_date(hvfhv_links, start_year, start_month, end_year, end_month)\n",
    "\n",
    "# Make full URLs if relative paths are present\n",
    "base_url = \"https://www.nyc.gov\"\n",
    "yellow_taxi_links = [\n",
    "    link if link.startswith(\"http\") else base_url + link for link in filtered_yellow_taxi_links\n",
    "]\n",
    "hvfhv_links = [\n",
    "    link if link.startswith(\"http\") else base_url + link for link in hvfhv_links\n",
    "]\n",
    "\n",
    "# Debugging: Print the found links\n",
    "print(\"Yellow Taxi Links:\", filtered_yellow_taxi_links)\n",
    "print(\"HVFHV Links:\", filtered_hvfhv_links)\n",
    "\n",
    "# Download Parquet files\n",
    "print(\"Downloading Yellow Taxi Parquet files...\")\n",
    "download_parquet_files(filtered_yellow_taxi_links, save_dir = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\")\n",
    "\n",
    "print(\"\\nDownloading HVFHV Parquet files...\")\n",
    "download_parquet_files(filtered_hvfhv_links, save_dir = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108987d7",
   "metadata": {},
   "source": [
    "#### 1.1.3 Loading the Taxi Zones shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Taxi Zones shapefile\n",
    "TAXI_ZONES_SHAPEFILE = \"taxi_zones.shp\"\n",
    "\n",
    "# Load the shapefile into a GeoDataFrame\n",
    "taxi_zones_gdf = gpd.read_file(TAXI_ZONES_SHAPEFILE)\n",
    "taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Inspect the GeoDataFrame\n",
    "print(taxi_zones_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706c68c",
   "metadata": {},
   "source": [
    "### 1.2 Related function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fed589",
   "metadata": {},
   "source": [
    "#### 1.2.1 Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> Optional[gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load and process the taxi zones shapefile, extracting longitude and latitude from the geometry column.\n",
    "\n",
    "    Parameters:\n",
    "        shapefile_path (str): Path to the taxi zones shapefile.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Processed GeoDataFrame with longitude and latitude columns added.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Taxi Zones GeoDataFrame\n",
    "        taxi_zones_gdf = gpd.read_file(shapefile)\n",
    "\n",
    "        # Check CRS and reproject to WGS84 (EPSG:4326) if necessary\n",
    "        if taxi_zones_gdf.crs != \"EPSG:4326\":\n",
    "            taxi_zones_gdf = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Extract longitude and latitude from the geometry column\n",
    "        taxi_zones_gdf[\"longitude\"] = taxi_zones_gdf.geometry.centroid.x\n",
    "        taxi_zones_gdf[\"latitude\"] = taxi_zones_gdf.geometry.centroid.y\n",
    "\n",
    "        return taxi_zones_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading and processing shapefile: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(\n",
    "    zone_loc_id: int, \n",
    "    loaded_taxi_zones: gpd.GeoDataFrame\n",
    ") -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Look up the coordinates (longitude, latitude) of a taxi zone given its Location ID.\n",
    "\n",
    "    Parameters:\n",
    "    - zone_loc_id (int): The Location ID of the taxi zone.\n",
    "    - loaded_taxi_zones (gpd.GeoDataFrame): The GeoDataFrame containing the taxi zones.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple (longitude, latitude) representing the coordinates of the taxi zone.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter the GeoDataFrame for the given Location ID\n",
    "        zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "\n",
    "        # Check if the zone is valid\n",
    "        if zone.empty:\n",
    "            raise ValueError(f\"Location ID {zone_loc_id} not found in the Taxi Zones dataset.\")\n",
    "\n",
    "        # Use the centroid of the zone polygon for coordinates\n",
    "        longitude = zone.geometry.centroid.x.values[0]\n",
    "        latitude = zone.geometry.centroid.y.values[0]\n",
    "\n",
    "        # Return as a tuple\n",
    "        return longitude, latitude\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error looking up coordinates for Location ID {zone_loc_id}: {e}\")\n",
    "        return None, None  # Return None for both longitude and latitude if there's an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b1522",
   "metadata": {},
   "source": [
    "#### 1.2.2 Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_taxi_sample_size(population: Union[int, float]) -> int:\n",
    "    '''\n",
    "    Calculate the sample size needed for a given population\n",
    "\n",
    "    Parameters:\n",
    "    - population: total population size\n",
    "\n",
    "    Returns:\n",
    "    - the sample size needed\n",
    "    '''\n",
    "    # set confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    z_score = 1.96 # for 95% confidence level\n",
    "    proportion = 0.5 # proportion of the population that has the attribute of interest (use 0.5 for max variance)\n",
    "\n",
    "    # implement Cochran's formula\n",
    "    sample_size = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "\n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f623cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fhvhv_sample_size(population: Union[int, float]) -> int:\n",
    "    '''\n",
    "    Calculate the sample size needed for a given population\n",
    "\n",
    "    Parameters:\n",
    "    - population: total population size\n",
    "\n",
    "    Returns:\n",
    "    - the sample size needed\n",
    "    '''\n",
    "    # set confidence_level = 0.99\n",
    "    margin_of_error = 0.05\n",
    "    z_score = 2.58 # for 95% confidence level\n",
    "    proportion = 0.5 # proportion of the population that has the attribute of interest (use 0.5 for max variance)\n",
    "\n",
    "    # implement Cochran's formula\n",
    "    sample_size = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "\n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bafdd",
   "metadata": {},
   "source": [
    "### 1.2.3 Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page: str) -> List[str]:\n",
    "    '''\n",
    "    Get all the URLs from the TLC page\n",
    "    \n",
    "    Parameters:\n",
    "    - taxi_page: the URL of the TLC page\n",
    "    \n",
    "    Returns:\n",
    "    - a list of all URLs on the page\n",
    "    '''\n",
    "    \n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    all_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c441edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls: List[str]) -> Dict[str, List[str]]:\n",
    "    '''\n",
    "    Filter the URLs to get only the Parquet files\n",
    "    \n",
    "    Parameters:\n",
    "    - all_urls: a list of all URLs\n",
    "    \n",
    "    Returns:\n",
    "    - a list of URLs that contain the word \"parquet\"\n",
    "    '''\n",
    "    \n",
    "    # Define patterns for Yellow Taxi and HVFHV Parquet files\n",
    "    yellow_taxi_pattern = r\".*yellow_tripdata.*\\.parquet\"\n",
    "    hvfhv_pattern = r\".*fhvhv_tripdata.*\\.parquet\"\n",
    "\n",
    "    # Filter URLs for each dataset\n",
    "    yellow_taxi_urls = [url for url in all_urls if re.search(yellow_taxi_pattern, url)]\n",
    "    hvfhv_urls = [url for url in all_urls if re.search(hvfhv_pattern, url)]\n",
    "\n",
    "    return {\n",
    "        \"yellow_taxi\": yellow_taxi_urls,\n",
    "        \"hvfhv\": hvfhv_urls,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9fe1c",
   "metadata": {},
   "source": [
    "### 1.3 Process Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e6db3",
   "metadata": {},
   "source": [
    "#### 1.3.1 Generate random samples of taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0449bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the taxi monthly datasets\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    if \"yellow\" in file_name and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # Read the dataset\n",
    "        data = pd.read_parquet(file_path)\n",
    "        population_size = len(data)  # Get total rows in the dataset\n",
    "\n",
    "        # Calculate sample size dynamically\n",
    "        sample_size = calculate_taxi_sample_size(population_size)\n",
    "\n",
    "        # Create a random sample\n",
    "        sampled_data = data.sample(n=sample_size)\n",
    "\n",
    "        # Save the sampled data to a new file\n",
    "        output_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_path)\n",
    "\n",
    "        # Print information for debugging\n",
    "        print(f\"Processed {file_name}: Population = {population_size}, Sample = {sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39480170",
   "metadata": {},
   "source": [
    "#### 1.3.2 Cleaning and processing taxi sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC bounding box coordinates (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LAT_MIN, LON_MIN = NEW_YORK_BOX_COORDS[0]\n",
    "LAT_MAX, LON_MAX = NEW_YORK_BOX_COORDS[1]\n",
    "CRS = 4326\n",
    "\n",
    "def get_and_clean_taxi_data(\n",
    "    file_path: str, \n",
    "    save_dir: str, \n",
    "    taxi_zones_gdf: gpd.GeoDataFrame\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean a single month's taxi data from a local Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        save_dir (str): Directory where cleaned files will be saved.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame of taxi zones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame for the given month.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the Parquet file\n",
    "        print(f\"Loading file: {file_name}...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "        # Step 3: Look up and add coordinates for pulocationid and dolocationid\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            valid_location_ids = taxi_zones_gdf[\"LocationID\"]\n",
    "            initial_rows = len(df)\n",
    "            df = df[df[\"pulocationid\"].isin(valid_location_ids) & df[\"dolocationid\"].isin(valid_location_ids)]\n",
    "            dropped_rows = initial_rows - len(df)\n",
    "            print(f\"Dropped {dropped_rows} rows with invalid Location IDs.\")\n",
    "        else:\n",
    "            print(f\"Columns 'pulocationid' or 'dolocationid' not found in {file_name}. Skipping location ID filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 5: Drop rows with the same pickup and dropoff location\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        filtered_rows = initial_rows - len(df)\n",
    "        print(f\"Filtered out {filtered_rows} rows with the same pickup and dropoff location.\")\n",
    "\n",
    "        # Step 6: Look up and add coordinates for pulocationid and dolocationid\n",
    "        print(\"Looking up coordinates for location IDs...\")\n",
    "        df[\"pickup_coords\"] = df[\"pulocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "        df[\"dropoff_coords\"] = df[\"dolocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "\n",
    "        # Split coordinates into latitude and longitude columns\n",
    "        df[[\"pickup_longitude\", \"pickup_latitude\"]] = pd.DataFrame(\n",
    "            df[\"pickup_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "        df[[\"dropoff_longitude\", \"dropoff_latitude\"]] = pd.DataFrame(\n",
    "            df[\"dropoff_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "\n",
    "        # Drop temporary coordinate columns\n",
    "        df.drop([\"pickup_coords\", \"dropoff_coords\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        # Step 7: Remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count',\n",
    "            'trip_distance', 'pulocationid', 'dolocationid', 'fare_amount', 'extra',\n",
    "            'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', \n",
    "            'congestion_surcharge', 'airport_fee', 'pickup_latitude', 'pickup_longitude',\n",
    "            'dropoff_latitude', 'dropoff_longitude'\n",
    "        ]\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Step 8: Normalize datetime columns\n",
    "        for time_col in ['tpep_pickup_datetime', 'tpep_dropoff_datetime']:\n",
    "            if time_col in df.columns:\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "                \n",
    "        if \"airport_fee\" in df.columns:\n",
    "            df[\"airport_fee\"] = df[\"airport_fee\"].fillna(0) \n",
    "            \n",
    "        # Step 9: Filter trips within NYC bounding box\n",
    "        if {\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"}.issubset(df.columns):\n",
    "            df = df[\n",
    "                (df['pickup_latitude'] >= LAT_MIN) & (df['pickup_latitude'] <= LAT_MAX) &\n",
    "                (df['pickup_longitude'] >= LON_MIN) & (df['pickup_longitude'] <= LON_MAX) &\n",
    "                (df['dropoff_latitude'] >= LAT_MIN) & (df['dropoff_latitude'] <= LAT_MAX) &\n",
    "                (df['dropoff_longitude'] >= LON_MIN) & (df['dropoff_longitude'] <= LON_MAX)\n",
    "            ]\n",
    "\n",
    "        # Step 10: Remove rides with the same pickup and dropoff locations or zero distance\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        if \"trip_distance\" in df.columns:\n",
    "            df = df[df[\"trip_distance\"] > 0]\n",
    "\n",
    "        # Step 11: Rename columns for clarity\n",
    "        df.rename(columns={\n",
    "            \"tpep_pickup_datetime\": \"pickup_time\",\n",
    "            \"tpep_dropoff_datetime\": \"dropoff_time\",\n",
    "            \"pulocationid\": \"pickup_location_id\",\n",
    "            \"dolocationid\": \"dropoff_location_id\",\n",
    "            \"extra\": \"miscellaneous_extra_charges\"\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        output_path = os.path.join(save_dir, f\"cleaned_{file_name}\")\n",
    "        df.to_parquet(output_path)\n",
    "        print(f\"Cleaned {len(df)} rows and saved to {output_path}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main Script\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load the Taxi Zones GeoDataFrame\n",
    "TAXI_ZONES_SHAPEFILE = os.path.join(directory, \"taxi_zones.shp\")\n",
    "taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "# Iterate over each Parquet file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    # Process only files that contain \"yellow\" and end with \".parquet\"\n",
    "    if \"sampled_yellow\" in file_name.lower() and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        get_and_clean_taxi_data(file_path, output_directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1428a",
   "metadata": {},
   "source": [
    "#### 1.3.3 Merge the cleaned .parquet files into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data(cleaned_files_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine cleaned taxi data from local cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all months.\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "\n",
    "    # Iterate through all cleaned Parquet files in the directory\n",
    "    for file_name in os.listdir(cleaned_files_dir):\n",
    "        if file_name.startswith(\"cleaned_\") and file_name.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(cleaned_files_dir, file_name)\n",
    "            print(f\"Loading cleaned file: {file_path}...\")\n",
    "            \n",
    "            # Load the cleaned Parquet file into a DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "            all_taxi_dataframes.append(df)\n",
    "\n",
    "    # Combine all cleaned data into a single DataFrame\n",
    "    if all_taxi_dataframes:\n",
    "        taxi_data = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "        print(f\"Combined {len(taxi_data)} rows from all cleaned files.\")\n",
    "    else:\n",
    "        taxi_data = pd.DataFrame()\n",
    "        print(\"No cleaned files found to process.\")\n",
    "\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_taxi_data(cleaned_files_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all cleaned Yellow Taxi and HVFHV data from cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all cleaned taxi data.\n",
    "    \"\"\"\n",
    "    # Process all cleaned Parquet files in the directory\n",
    "    print(\"Combining all cleaned taxi data...\")\n",
    "    all_data = get_taxi_data(cleaned_files_dir=cleaned_files_dir)\n",
    "\n",
    "    print(f\"Final combined data contains {len(all_data)} rows.\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_FILES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Load and combine all cleaned data into a single DataFrame\n",
    "taxi_data = get_all_taxi_data(cleaned_files_dir=CLEANED_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### 1.4 Processing Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648f47b",
   "metadata": {},
   "source": [
    "#### 1.4.1 Generate random samples of uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1856e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the taxi monthly datasets\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    if \"fhvhv\" in file_name and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # Read the dataset\n",
    "        data = pd.read_parquet(file_path)\n",
    "        population_size = len(data)  # Get total rows in the dataset\n",
    "\n",
    "        # Calculate sample size dynamically\n",
    "        sample_size = calculate_fhvhv_sample_size(population_size)\n",
    "\n",
    "        # Create a random sample\n",
    "        sampled_data = data.sample(n=sample_size)\n",
    "\n",
    "        # Save the sampled data to a new file\n",
    "        output_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_path)\n",
    "\n",
    "        # Print information for debugging\n",
    "        print(f\"Processed {file_name}: Population = {population_size}, Sample = {sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee46796",
   "metadata": {},
   "source": [
    "#### 1.4.2 Cleaning and processing uber sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(\n",
    "    file_path: str, \n",
    "    save_dir: str, \n",
    "    taxi_zones_gdf: gpd.GeoDataFrame\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean a single month's Uber data from a local Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        save_dir (str): Directory where cleaned files will be saved.\n",
    "        taxi_zones_gdf (gpd.GeoDataFrame): GeoDataFrame of taxi zones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame for the given month.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the Parquet file\n",
    "        print(f\"Loading file: {file_name}...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Step 2: Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "        # Step 3: Filter out non-Uber rides\n",
    "        if \"hvfhs_license_num\" in df.columns:\n",
    "            df = df[df[\"hvfhs_license_num\"].str.contains(\"HV0003\", na=False)]\n",
    "            print(f\"Filtered to Uber rides only: {len(df)} rows remaining.\")\n",
    "        else:\n",
    "            print(f\"'hvfhs_license_num' column not found in {file_name}. Skipping Uber filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 4: Drop rows with invalid Location IDs\n",
    "        if \"pulocationid\" in df.columns and \"dolocationid\" in df.columns:\n",
    "            valid_location_ids = set(taxi_zones_gdf[\"LocationID\"])\n",
    "            initial_rows = len(df)\n",
    "            df = df[df[\"pulocationid\"].isin(valid_location_ids) & df[\"dolocationid\"].isin(valid_location_ids)]\n",
    "            dropped_rows = initial_rows - len(df)\n",
    "            print(f\"Dropped {dropped_rows} rows with invalid Location IDs.\")\n",
    "        else:\n",
    "            print(f\"Columns 'pulocationid' or 'dolocationid' not found in {file_name}. Skipping location ID filtering.\")\n",
    "            return None\n",
    "\n",
    "        # Step 5: Drop rows with the same pickup and dropoff location\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"pulocationid\"] != df[\"dolocationid\"]]\n",
    "        filtered_rows = initial_rows - len(df)\n",
    "        print(f\"Filtered out {filtered_rows} rows with the same pickup and dropoff location.\")\n",
    "\n",
    "        # Step 6: Look up and add coordinates for pulocationid and dolocationid\n",
    "        print(\"Looking up coordinates for location IDs...\")\n",
    "        df[\"pickup_coords\"] = df[\"pulocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "        df[\"dropoff_coords\"] = df[\"dolocationid\"].apply(\n",
    "            lambda loc_id: lookup_coords_for_taxi_zone_id(loc_id, taxi_zones_gdf)\n",
    "        )\n",
    "\n",
    "        # Split coordinates into latitude and longitude columns\n",
    "        df[[\"pickup_longitude\", \"pickup_latitude\"]] = pd.DataFrame(\n",
    "            df[\"pickup_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "        df[[\"dropoff_longitude\", \"dropoff_latitude\"]] = pd.DataFrame(\n",
    "            df[\"dropoff_coords\"].tolist(), index=df.index\n",
    "        )\n",
    "\n",
    "        # Drop temporary coordinate columns\n",
    "        df.drop([\"pickup_coords\", \"dropoff_coords\"], axis=1, inplace=True)\n",
    "\n",
    "        # Step 7: Remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid',\n",
    "            'trip_miles', 'base_passenger_fare', 'tolls', 'sales_tax', \n",
    "            'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
    "            'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'\n",
    "        ]\n",
    "        df = df[[col for col in columns_to_keep if col in df.columns]]\n",
    "\n",
    "        # Step 8: Normalize datetime columns\n",
    "        for time_col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            if time_col in df.columns:\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "        if 'trip_time' in df.columns:\n",
    "            df['trip_time'] = pd.to_timedelta(df['trip_time'], errors='coerce')\n",
    "\n",
    "        if \"airport_fee\" in df.columns:\n",
    "            df[\"airport_fee\"] = df[\"airport_fee\"].fillna(0) \n",
    "\n",
    "        # Step 9: Filter trips within NYC bounding box\n",
    "        if {\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"}.issubset(df.columns):\n",
    "            df = df[\n",
    "                (df['pickup_latitude'] >= LAT_MIN) & (df['pickup_latitude'] <= LAT_MAX) &\n",
    "                (df['pickup_longitude'] >= LON_MIN) & (df['pickup_longitude'] <= LON_MAX) &\n",
    "                (df['dropoff_latitude'] >= LAT_MIN) & (df['dropoff_latitude'] <= LAT_MAX) &\n",
    "                (df['dropoff_longitude'] >= LON_MIN) & (df['dropoff_longitude'] <= LON_MAX)\n",
    "            ]\n",
    "\n",
    "        # Step 10: Rename columns for clarity\n",
    "        df.rename(columns={\n",
    "            \"pickup_datetime\": \"pickup_time\",\n",
    "            \"dropoff_datetime\": \"dropoff_time\",\n",
    "            \"pulocationid\": \"pickup_location_id\",\n",
    "            \"dolocationid\": \"dropoff_location_id\",\n",
    "            \"trip_miles\": \"trip_distance\",\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Save the cleaned data\n",
    "        output_path = os.path.join(save_dir, f\"cleaned_{file_name}\")\n",
    "        df.to_parquet(output_path)\n",
    "        print(f\"Cleaned {len(df)} rows and saved to {output_path}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Script\n",
    "directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "output_directory = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load the Taxi Zones GeoDataFrame\n",
    "TAXI_ZONES_SHAPEFILE = os.path.join(directory, \"taxi_zones.shp\")\n",
    "taxi_zones_gdf = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "# Iterate over each Parquet file in the directory\n",
    "for file_name in os.listdir(directory):\n",
    "    # Process only files that contain \"yellow\" and end with \".parquet\"\n",
    "    if \"sampled_fhvhv\" in file_name.lower() and file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        get_and_clean_uber_data(file_path, output_directory, taxi_zones_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e6996",
   "metadata": {},
   "source": [
    "#### 1.4.3 Merge the cleaned .parquet files into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data(\n",
    "    cleaned_files_dir: str, \n",
    "    prefix: str = \"cleaned_sampled_fhvhv\", \n",
    "    file_extension: str = \".parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine cleaned Uber data from local cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "        prefix (str): Prefix to identify Uber files (default is \"cleaned_uber_\").\n",
    "        file_extension (str): File extension to identify Parquet files (default is \".parquet\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all months.\n",
    "    \"\"\"\n",
    "    all_uber_dataframes = []\n",
    "\n",
    "    # Verify that the directory exists\n",
    "    if not os.path.exists(cleaned_files_dir):\n",
    "        print(f\"Directory does not exist: {cleaned_files_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in os.listdir(cleaned_files_dir):\n",
    "        # Filter files by prefix and file extension\n",
    "        if file_name.startswith(prefix) and file_name.endswith(file_extension):\n",
    "            file_path = os.path.join(cleaned_files_dir, file_name)\n",
    "            print(f\"Loading cleaned Uber file: {file_path}...\")\n",
    "            \n",
    "            # Load the cleaned Parquet file into a DataFrame\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                all_uber_dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "    # Combine all cleaned data into a single DataFrame\n",
    "    if all_uber_dataframes:\n",
    "        uber_data = pd.concat(all_uber_dataframes, ignore_index=True)\n",
    "        print(f\"Combined {len(uber_data)} rows from all cleaned Uber files.\")\n",
    "    else:\n",
    "        uber_data = pd.DataFrame()\n",
    "        print(\"No cleaned Uber files found to process.\")\n",
    "\n",
    "    return uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_uber_data(\n",
    "    cleaned_files_dir: str, \n",
    "    prefix: str = \"cleaned_sampled_fhvhv\", \n",
    "    file_extension: str = \".parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all cleaned Uber data from cleaned Parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_files_dir (str): Directory where cleaned Parquet files are stored.\n",
    "        prefix (str): Prefix to identify Uber files (default is \"cleaned_uber_\").\n",
    "        file_extension (str): File extension to identify Parquet files (default is \".parquet\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all cleaned Uber data.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(cleaned_files_dir):\n",
    "        print(f\"Directory does not exist: {cleaned_files_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Combine all cleaned Uber data\n",
    "    print(f\"Combining all cleaned Uber data from directory: {cleaned_files_dir}...\")\n",
    "    all_uber_data = get_uber_data(cleaned_files_dir=cleaned_files_dir, prefix=prefix, file_extension=file_extension)\n",
    "\n",
    "    # Check if the resulting DataFrame is empty\n",
    "    if all_uber_data.empty:\n",
    "        print(\"No Uber data found or combined. Returning an empty DataFrame.\")\n",
    "    else:\n",
    "        print(f\"Final combined Uber data contains {len(all_uber_data)} rows.\")\n",
    "    \n",
    "    return all_uber_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script for Uber Data\n",
    "CLEANED_UBER_FILES_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "\n",
    "# Load and combine all cleaned Uber data into a single DataFrame\n",
    "uber_data = get_all_uber_data(cleaned_files_dir=CLEANED_UBER_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95c131",
   "metadata": {},
   "source": [
    "### 1.5 Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b236a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of all CSV files in the given directory that are related to weather data.\n",
    "    Weather-related files are identified by the presence of the keyword 'weather' in the filename.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory to search for files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of weather-related CSV filenames.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"Invalid directory: {directory}\")\n",
    "    \n",
    "    weather_csvs = [\n",
    "        file for file in os.listdir(directory)\n",
    "        if file.endswith('.csv') and 'weather' in file.lower()\n",
    "    ]\n",
    "    \n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb4263",
   "metadata": {},
   "source": [
    "#### 1.5.1 Cleaning and processing weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d84a9f",
   "metadata": {},
   "source": [
    "##### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the weather dataset by:\n",
    "    - Extracting specific columns.\n",
    "    - Normalizing column names by adding underscores between words.\n",
    "    - Converting weather codes to descriptive terms.\n",
    "    - Handling multiple data points within the same hour by selecting the ideal one.\n",
    "    - Filling missing hourly data by interpolating between previous and next data points.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file containing the weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with selected and transformed columns.\n",
    "    \"\"\"\n",
    "    # AU code mapping for HourlyPresentWeatherType\n",
    "    au_code_mapping = {\n",
    "        \"DZ\": \"Drizzle\",\n",
    "        \"RA\": \"Rain\",\n",
    "        \"SN\": \"Snow\",\n",
    "        \"SG\": \"Snow Grains\",\n",
    "        \"IC\": \"Ice Crystals\",\n",
    "        \"PL\": \"Ice Pellets\",\n",
    "        \"GR\": \"Hail\",\n",
    "        \"GS\": \"Small Hail\",\n",
    "        \"UP\": \"Unknown Precipitation\",\n",
    "        \"BR\": \"Mist\",\n",
    "        \"FG\": \"Fog\",\n",
    "        \"FU\": \"Smoke\",\n",
    "        \"VA\": \"Volcanic Ash\",\n",
    "        \"DU\": \"Dust\",\n",
    "        \"SA\": \"Sand\",\n",
    "        \"HZ\": \"Haze\",\n",
    "        \"PY\": \"Spray\",\n",
    "        \"PO\": \"Sand Whirls\",\n",
    "        \"SQ\": \"Squalls\",\n",
    "        \"FC\": \"Funnel Cloud\",\n",
    "        \"SS\": \"Sandstorm\",\n",
    "        \"DS\": \"Duststorm\"\n",
    "    }\n",
    "\n",
    "    # Sky condition mapping for HourlySkyConditions\n",
    "    sky_condition_mapping = {\n",
    "        \"CLR\": \"Clear\",\n",
    "        \"FEW\": \"Few Clouds\",\n",
    "        \"SCT\": \"Scattered Clouds\",\n",
    "        \"BKN\": \"Broken Clouds\",\n",
    "        \"OVC\": \"Overcast\",\n",
    "        \"VV\": \"Obscured Sky\"\n",
    "    }\n",
    "\n",
    "    # Function to interpret HourlyPresentWeatherType using AU codes\n",
    "    def interpret_weather_type(weather_string):\n",
    "        if pd.isnull(weather_string):\n",
    "            return \"Unknown\"\n",
    "        matches = re.findall(r\"([A-Z]{2}):\\d+\", weather_string)\n",
    "        descriptions = [au_code_mapping.get(code, \"Unknown\") for code in matches]\n",
    "        return \", \".join(set(descriptions)) if descriptions else \"Unknown\"\n",
    "\n",
    "    # Function to interpret HourlySkyConditions\n",
    "    def interpret_sky_conditions(sky_string):\n",
    "        if pd.isnull(sky_string):\n",
    "            return \"Unknown\"\n",
    "        pattern = r\"(\\w{3}):(\\d{2})(?:\\s(\\d+))?\"\n",
    "        matches = re.findall(pattern, sky_string)\n",
    "        interpreted_conditions = []\n",
    "        for condition, octa, elevation in matches:\n",
    "            description = sky_condition_mapping.get(condition, \"Unknown\")\n",
    "            detail = f\"{description}, Octas: {int(octa)}\"\n",
    "            if elevation:\n",
    "                detail += f\", Elevation: {int(elevation)} feet\"\n",
    "            interpreted_conditions.append(detail)\n",
    "        return \"; \".join(interpreted_conditions) if interpreted_conditions else \"Unknown\"\n",
    "\n",
    "    # Function to select the ideal row for each hour\n",
    "    def select_ideal_row(group):\n",
    "        if group.empty:\n",
    "            return None\n",
    "        if len(group) == 1:\n",
    "            return group.iloc[0]\n",
    "        middle_of_hour = group.index[0].replace(minute=30, second=0, microsecond=0)\n",
    "        time_diffs = abs((group.index - middle_of_hour).total_seconds())\n",
    "        return group.iloc[time_diffs.argmin()]\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    # Normalize column names (add underscores between words and make lowercase)\n",
    "    df.columns = df.columns.str.replace(r\"([a-z])([A-Z])\", r\"\\1_\\2\", regex=True).str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    column_renaming = {\n",
    "        \"date\": \"hourly_time\",\n",
    "        \"hourly_dry_bulb_temperature\": \"hourly_temperature\"\n",
    "    }\n",
    "    df.rename(columns=column_renaming, inplace=True)\n",
    "\n",
    "    # Select only the required columns\n",
    "    columns_to_extract = [\n",
    "        \"hourly_time\", \"hourly_temperature\", \"hourly_present_weather_type\", \n",
    "        \"hourly_sky_conditions\", \"hourly_visibility\", \"hourly_precipitation\", \"hourly_wind_speed\"\n",
    "    ]\n",
    "    df = df[[col for col in columns_to_extract if col in df.columns]]\n",
    "\n",
    "    # Convert hourly_time to datetime\n",
    "    df[\"hourly_time\"] = pd.to_datetime(df[\"hourly_time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"hourly_time\"])  # Drop rows with invalid dates\n",
    "    df = df.sort_values(\"hourly_time\")  # Sort by hourly_time\n",
    "    df.set_index(\"hourly_time\", inplace=True)  # Set hourly_time as the index\n",
    "\n",
    "    # Group by hourly and select the ideal row\n",
    "    df_hourly = df.groupby(pd.Grouper(freq='H')).apply(select_ideal_row)\n",
    "    df_hourly = df_hourly.dropna()  # Drop any None rows from empty groups\n",
    "\n",
    "    # Reindex to include all hourly intervals and interpolate missing data\n",
    "    all_hours = pd.date_range(start=df_hourly.index.min(), end=df_hourly.index.max(), freq='H')\n",
    "    df_hourly = df_hourly.reindex(all_hours)\n",
    "    df_hourly.index.name = \"hourly_time\"\n",
    "\n",
    "    # Replace NaN in hourly_precipitation with 0, and handle 'T' values\n",
    "    if \"hourly_precipitation\" in df_hourly.columns:\n",
    "    # Replace known non-numeric values\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].replace(\"T\", \"0.01\")\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].replace(\"\", \"0\")\n",
    "    # Remove any leftover non-numeric characters (e.g., \"s\")\n",
    "        df_hourly[\"hourly_precipitation\"] = df_hourly[\"hourly_precipitation\"].str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "    # Convert to float, coercing any remaining errors to NaN\n",
    "        df_hourly[\"hourly_precipitation\"] = pd.to_numeric(df_hourly[\"hourly_precipitation\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "    # Apply forward-fill for hourly_wind_speed -- assume wind doesn't change much within an hour\n",
    "    if \"hourly_wind_speed\" in df_hourly.columns:\n",
    "        df_hourly[\"hourly_wind_speed\"] = df_hourly[\"hourly_wind_speed\"].fillna(method='ffill')\n",
    "\n",
    "    # Convert numerical columns to numeric, coercing errors to NaN\n",
    "    numerical_cols = [\"hourly_temperature\", \"hourly_visibility\"]\n",
    "    for col in numerical_cols:\n",
    "        df_hourly[col] = pd.to_numeric(df_hourly[col], errors=\"coerce\")\n",
    "\n",
    "    # Interpolate numerical columns\n",
    "    df_hourly[numerical_cols] = df_hourly[numerical_cols].interpolate(method='time')\n",
    "\n",
    "    # Fill categorical columns with previous value or next value if previous is missing\n",
    "    categorical_cols = [\"hourly_present_weather_type\", \"hourly_sky_conditions\"]\n",
    "    df_hourly[categorical_cols] = df_hourly[categorical_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Reset index to bring hourly_time back as a column\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    # Convert hourly_present_weather_type to descriptive terms\n",
    "    df_hourly[\"hourly_present_weather_type\"] = df_hourly[\"hourly_present_weather_type\"].apply(interpret_weather_type)\n",
    "\n",
    "    # Convert hourly_sky_conditions to descriptive terms\n",
    "    df_hourly[\"hourly_sky_conditions\"] = df_hourly[\"hourly_sky_conditions\"].apply(interpret_sky_conditions)\n",
    "\n",
    "    return df_hourly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e112f",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the weather dataset for daily data by:\n",
    "    - Extracting specific columns.\n",
    "    - Interpreting the DailyWeather column using AU codes.\n",
    "    - Converting Sunrise and Sunset columns to time format.\n",
    "    - Replacing None in Sunrise and Sunset columns with the previous day's value.\n",
    "    - Removing rows with NaN values.\n",
    "    - Normalizing column names by adding underscores between words.\n",
    "    - Replacing 'dry_bulb_temperature' with 'temperature' in column names.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file containing the weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with selected and transformed columns.\n",
    "    \"\"\"\n",
    "    # AU code mapping for DailyWeather\n",
    "    au_code_mapping = {\n",
    "        \"DZ\": \"Drizzle\",\n",
    "        \"RA\": \"Rain\",\n",
    "        \"SN\": \"Snow\",\n",
    "        \"SG\": \"Snow Grains\",\n",
    "        \"IC\": \"Ice Crystals\",\n",
    "        \"PL\": \"Ice Pellets\",\n",
    "        \"GR\": \"Hail\",\n",
    "        \"GS\": \"Small Hail\",\n",
    "        \"UP\": \"Unknown Precipitation\",\n",
    "        \"BR\": \"Mist\",\n",
    "        \"FG\": \"Fog\",\n",
    "        \"FU\": \"Smoke\",\n",
    "        \"VA\": \"Volcanic Ash\",\n",
    "        \"DU\": \"Dust\",\n",
    "        \"SA\": \"Sand\",\n",
    "        \"HZ\": \"Haze\",\n",
    "        \"PY\": \"Spray\",\n",
    "        \"PO\": \"Sand Whirls\",\n",
    "        \"SQ\": \"Squalls\",\n",
    "        \"FC\": \"Funnel Cloud\",\n",
    "        \"SS\": \"Sandstorm\",\n",
    "        \"DS\": \"Duststorm\"\n",
    "    }\n",
    "\n",
    "    # Function to interpret DailyWeather using AU codes\n",
    "    def interpret_daily_weather(weather_string):\n",
    "        if pd.isnull(weather_string):\n",
    "            return \"Unknown\"\n",
    "        matches = re.findall(r\"([A-Z]{2})[:\\d]*\", weather_string)\n",
    "        descriptions = [au_code_mapping.get(code, \"Unknown\") for code in matches]\n",
    "        return \", \".join(set(descriptions)) if descriptions else \"Unknown\"\n",
    "\n",
    "    # Function to convert time in integer format (e.g., 720 -> 7:20 am)\n",
    "    def convert_to_time_format(time_int):\n",
    "        if pd.isnull(time_int):\n",
    "            return None\n",
    "        try:\n",
    "            time_str = f\"{int(time_int):04d}\"  # Ensure it's always 4 digits (e.g., 720 -> \"0720\")\n",
    "            return datetime.strptime(time_str, \"%H%M\").time()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    # Select only the required columns\n",
    "    columns_to_extract = [\n",
    "        \"DATE\", \"Sunrise\", \"Sunset\", \"DailyAverageDryBulbTemperature\", \n",
    "        \"DailyAverageWindSpeed\", \"DailyMaximumDryBulbTemperature\", \n",
    "        \"DailyMinimumDryBulbTemperature\", \"DailyWeather\", \"DailySnowDepth\"\n",
    "    ]\n",
    "    df = df[[col for col in columns_to_extract if col in df.columns]]\n",
    "\n",
    "    # Convert DATE to datetime\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DATE\"])  # Drop rows with invalid dates\n",
    "    df = df.sort_values(\"DATE\")  # Sort by date\n",
    "\n",
    "    # Convert Sunrise and Sunset columns to time format\n",
    "    if \"Sunrise\" in df.columns:\n",
    "        df[\"Sunrise\"] = df[\"Sunrise\"].apply(convert_to_time_format)\n",
    "    if \"Sunset\" in df.columns:\n",
    "        df[\"Sunset\"] = df[\"Sunset\"].apply(convert_to_time_format)\n",
    "\n",
    "    # Replace None in Sunrise and Sunset columns with the previous day's value\n",
    "    df[\"Sunrise\"] = df[\"Sunrise\"].fillna(method='ffill')\n",
    "    df[\"Sunset\"] = df[\"Sunset\"].fillna(method='ffill')\n",
    "\n",
    "    # Interpret DailyWeather column\n",
    "    if \"DailyWeather\" in df.columns:\n",
    "        df[\"DailyWeather\"] = df[\"DailyWeather\"].apply(interpret_daily_weather)\n",
    "\n",
    "    # Convert DailySnowDepth column to numeric and handle 'T' values\n",
    "    if \"DailySnowDepth\" in df.columns:\n",
    "        df[\"DailySnowDepth\"] = df[\"DailySnowDepth\"].replace(\"T\", 0.01).astype(float)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Rename columns to use underscores between words\n",
    "    df.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower() for col in df.columns]\n",
    "\n",
    "    # Replace 'dry_bulb_temperature' with 'temperature' in column names\n",
    "    df.columns = [col.replace(\"dry_bulb_temperature\", \"temperature\") for col in df.columns]\n",
    "\n",
    "    df = df.rename(columns={\"d_a_t_e\": \"date\"})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_CSV_DIR = \"/Users/victorzhang/Desktop/Columbia S1/IEOR E4501 Tools for Analytics/Final Project\"\n",
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather \n",
    "(\n",
    "    hourly_time DATETIME PRIMARY KEY,\n",
    "    hourly_temperature FLOAT,\n",
    "    hourly_present_weather_type STRING,\n",
    "    hourly_sky_conditions STRING,\n",
    "    hourly_visibility FLOAT,\n",
    "    hourly_precipitation FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daliy_weather \n",
    "(\n",
    "    date DATETIME PRIMARY KEY,\n",
    "    sunrise TIME,\n",
    "    sunset TIME,\n",
    "    daily_average_temperature FLOAT,\n",
    "    daily_average_wind_speed FLOAT,\n",
    "    daily_maximum_temperature FLOAT,\n",
    "    daily_minimum_temperature FLOAT,\n",
    "    daily_weather STRING,\n",
    "    daily_snow_depth FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips \n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_time DATETIME,\n",
    "    dropoff_time DATETIME,\n",
    "    passenger_count INTEGER,\n",
    "    trip_distance FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    miscellaneous_extra_charges FLOAT,\n",
    "    mta_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    improvement_surcharge FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips \n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_time DATETIME,\n",
    "    dropoff_time DATETIME,\n",
    "    pickup_location_id INTEGER,\n",
    "    dropoff_location_id INTEGER,\n",
    "    trip_distance FLOAT,\n",
    "    base_passenger_fare FLOAT,\n",
    "    tolls FLOAT,\n",
    "    sales_tax FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    tips FLOAT,\n",
    "    driver_pay FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables using the updated schemas\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(HOURLY_WEATHER_SCHEMA))\n",
    "    connection.execute(text(DAILY_WEATHER_SCHEMA))\n",
    "    connection.execute(text(TAXI_TRIPS_SCHEMA))\n",
    "    connection.execute(text(UBER_TRIPS_SCHEMA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(\n",
    "    table_to_df_dict: Dict[str, pd.DataFrame], \n",
    "    engine: Engine, \n",
    "    if_exists: str = \"replace\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write multiple DataFrames to a database as tables.\n",
    "\n",
    "    Args:\n",
    "        table_to_df_dict (Dict[str, pd.DataFrame]): A dictionary where keys are table names\n",
    "                                                    and values are DataFrames to be written.\n",
    "        engine (Engine): SQLAlchemy database engine.\n",
    "        if_exists (str): Behavior if the table already exists. Options are 'fail', 'replace', or 'append'.\n",
    "                         Default is 'replace'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        try:\n",
    "            df.to_sql(table_name, con=engine, if_exists=if_exists, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing table '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes a SQL query to a specified file.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The SQL query string to write to the file.\n",
    "        filename (str): The name of the file where the query will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(query)\n",
    "        print(f\"Query successfully written to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"query1.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    STRFTIME('%H', pickup_time) AS hour_of_day,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM taxi_trips\n",
    "WHERE pickup_time BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY hour_of_day\n",
    "ORDER BY ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7478b",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fe001",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"query2.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_time) AS day_of_week, \n",
    "    COUNT(*) AS ride_count\n",
    "FROM uber_trips\n",
    "WHERE pickup_time BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY day_of_week\n",
    "ORDER BY ride_count DESC; \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c454f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_2)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_2, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b6caf",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f31734",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"query3.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH filtered_trips AS (\n",
    "    SELECT \n",
    "        trip_distance\n",
    "    FROM \n",
    "        uber_trips\n",
    "    WHERE \n",
    "        DATE(pickup_time) BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "),\n",
    "percentile_calculation AS (\n",
    "    SELECT \n",
    "        trip_distance,\n",
    "        ROW_NUMBER() OVER (ORDER BY trip_distance) AS row_num,\n",
    "        COUNT(*) OVER () AS total_rows\n",
    "    FROM \n",
    "        filtered_trips\n",
    ")\n",
    "SELECT \n",
    "    trip_distance\n",
    "FROM \n",
    "    percentile_calculation\n",
    "WHERE \n",
    "    row_num = CAST(total_rows * 0.95 AS INTEGER);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_3)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_3, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117317b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522a584",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"query4.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH ride_counts AS (\n",
    "    SELECT\n",
    "        DATE(pickup_time) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_distance) AS avg_distance\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE\n",
    "        pickup_time BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY\n",
    "        DATE(pickup_time)\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        DATE(pickup_time) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_distance) AS avg_distance\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE\n",
    "        pickup_time BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY\n",
    "        DATE(pickup_time)\n",
    "),\n",
    "combined_counts AS (\n",
    "    SELECT\n",
    "        ride_date,\n",
    "        SUM(total_rides) AS total_rides,\n",
    "        AVG(avg_distance) AS avg_distance\n",
    "    FROM\n",
    "        ride_counts\n",
    "    GROUP BY\n",
    "        ride_date\n",
    "),\n",
    "top_10_days AS (\n",
    "    SELECT\n",
    "        ride_date,\n",
    "        total_rides,\n",
    "        avg_distance\n",
    "    FROM\n",
    "        combined_counts\n",
    "    ORDER BY\n",
    "        total_rides DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT\n",
    "    t.ride_date,\n",
    "    t.total_rides,\n",
    "    t.avg_distance,\n",
    "    AVG(w.hourly_precipitation) AS avg_precipitation,\n",
    "    AVG(w.hourly_wind_speed) AS avg_wind_speed\n",
    "FROM\n",
    "    top_10_days t\n",
    "JOIN\n",
    "    hourly_weather w\n",
    "ON\n",
    "    DATE(w.hourly_time) = t.ride_date\n",
    "GROUP BY\n",
    "    t.ride_date, t.total_rides, t.avg_distance\n",
    "ORDER BY\n",
    "    t.total_rides DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b09221",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_4)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_4, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd79ae",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"query5.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH snow_days AS (\n",
    "    SELECT\n",
    "        DATE(date) AS snow_date,\n",
    "        SUM(daily_snow_depth) AS total_snowfall\n",
    "    FROM\n",
    "        daily_weather\n",
    "    WHERE\n",
    "        date BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "        AND daily_snow_depth > 0\n",
    "    GROUP BY\n",
    "        DATE(date)\n",
    "),\n",
    "uber_rides AS (\n",
    "    SELECT\n",
    "        DATE(pickup_time) AS ride_date,\n",
    "        COUNT(*) AS total_uber_rides\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE\n",
    "        pickup_time BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "    GROUP BY\n",
    "        DATE(pickup_time)\n",
    "),\n",
    "taxi_rides AS (\n",
    "    SELECT\n",
    "        DATE(pickup_time) AS ride_date,\n",
    "        COUNT(*) AS total_taxi_rides\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE\n",
    "        pickup_time BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "    GROUP BY\n",
    "        DATE(pickup_time)\n",
    "),\n",
    "total_rides AS (\n",
    "    SELECT\n",
    "        COALESCE(u.ride_date, t.ride_date) AS ride_date,\n",
    "        COALESCE(u.total_uber_rides, 0) + COALESCE(t.total_taxi_rides, 0) AS total_rides\n",
    "    FROM\n",
    "        uber_rides u\n",
    "    FULL OUTER JOIN\n",
    "        taxi_rides t\n",
    "    ON\n",
    "        u.ride_date = t.ride_date\n",
    ")\n",
    "SELECT\n",
    "    s.snow_date,\n",
    "    s.total_snowfall,\n",
    "    COALESCE(r.total_rides, 0) AS total_rides\n",
    "FROM\n",
    "    snow_days s\n",
    "LEFT JOIN\n",
    "    total_rides r\n",
    "ON\n",
    "    s.snow_date = r.ride_date\n",
    "ORDER BY\n",
    "    s.total_snowfall DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_5)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_5, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd55255",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c1034",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93112b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"query6.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH RECURSIVE dates(x) AS (\n",
    "    SELECT '2023-09-25 00:00:00' -- Start date and time\n",
    "    UNION ALL\n",
    "    SELECT datetime(x, '+1 hour')\n",
    "    FROM dates\n",
    "    WHERE x < '2023-10-03 23:00:00' -- End date and time\n",
    "),\n",
    "uber_hourly_rides AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H:00:00', pickup_time) AS hourly_time,\n",
    "        COUNT(*) AS uber_rides\n",
    "    FROM \n",
    "        uber_trips\n",
    "    WHERE \n",
    "        pickup_time >= '2023-09-25 00:00:00' AND pickup_time <= '2023-10-03 23:59:59'\n",
    "    GROUP BY \n",
    "        hourly_time\n",
    "),\n",
    "taxi_hourly_rides AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H:00:00', pickup_time) AS hourly_time,\n",
    "        COUNT(*) AS taxi_rides\n",
    "    FROM \n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        pickup_time >= '2023-09-25 00:00:00' AND pickup_time <= '2023-10-03 23:59:59'\n",
    "    GROUP BY \n",
    "        hourly_time\n",
    "),\n",
    "hourly_rides AS (\n",
    "    SELECT \n",
    "        COALESCE(u.hourly_time, t.hourly_time) AS hourly_time,\n",
    "        COALESCE(u.uber_rides, 0) + COALESCE(t.taxi_rides, 0) AS num_rides\n",
    "    FROM \n",
    "        uber_hourly_rides u\n",
    "    FULL OUTER JOIN \n",
    "        taxi_hourly_rides t\n",
    "    ON \n",
    "        u.hourly_time = t.hourly_time\n",
    "),\n",
    "hourly_weather_data AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H:00:00', hourly_time) AS hourly_time,\n",
    "        SUM(hourly_precipitation) AS total_precipitation,\n",
    "        AVG(hourly_wind_speed) AS avg_wind_speed\n",
    "    FROM \n",
    "        hourly_weather\n",
    "    WHERE \n",
    "        hourly_time BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY \n",
    "        hourly_time\n",
    "),\n",
    "combined_data AS (\n",
    "    SELECT \n",
    "        d.x AS hourly_time,\n",
    "        COALESCE(r.num_rides, 0) AS num_rides,\n",
    "        COALESCE(w.total_precipitation, 0) AS total_precipitation,\n",
    "        COALESCE(w.avg_wind_speed, 0) AS avg_wind_speed\n",
    "    FROM \n",
    "        dates d\n",
    "    LEFT JOIN \n",
    "        hourly_rides r\n",
    "    ON \n",
    "        d.x = r.hourly_time\n",
    "    LEFT JOIN \n",
    "        hourly_weather_data w\n",
    "    ON \n",
    "        d.x = w.hourly_time\n",
    ")\n",
    "SELECT \n",
    "    hourly_time,\n",
    "    num_rides,\n",
    "    total_precipitation,\n",
    "    avg_wind_speed\n",
    "FROM \n",
    "    combined_data\n",
    "ORDER BY \n",
    "    hourly_time ASC;\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_6)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_6, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3eb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for the first visualization\n",
    "def get_data_for_visual_1():\n",
    "    return pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_popular_hour_for_taxis(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Create a bar chart to show the most popular hour to take a taxi, sorted from highest to lowest.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Contains columns 'hour_of_day' and 'ride_count'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by 'ride_count' in descending order\n",
    "    sorted_data = dataframe.sort_values(by=\"ride_count\", ascending=False)\n",
    "\n",
    "    # Extract data from the sorted DataFrame\n",
    "    hours = sorted_data[\"hour_of_day\"].astype(str)  # Convert hours to strings for categorical x-axis\n",
    "    ride_counts = sorted_data[\"ride_count\"]\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    # Create the bar plot\n",
    "    ax.bar(hours, ride_counts, color=\"skyblue\")\n",
    "\n",
    "    # Add labels, title, and ticks\n",
    "    ax.set_title(\"Most Popular Hour to Take a Taxi (2020-2024)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Hour of the Day (24-hour format, sorted)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of Rides\", fontsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  # Add grid lines for better readability\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xticks(fontsize=12, rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1 = get_data_for_visual_1()\n",
    "plot_most_popular_hour_for_taxis(v_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cfd142",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the second visualization\n",
    "def get_combined_monthly_distance_data(\n",
    "    uber_data: pd.DataFrame, \n",
    "    taxi_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine Uber and Taxi trip data, filter by date range, and calculate the total distance traveled per month.\n",
    "\n",
    "    Args:\n",
    "        uber_data (pd.DataFrame): DataFrame containing Uber trip data with 'pickup_time' and 'trip_distance' columns.\n",
    "        taxi_data (pd.DataFrame): DataFrame containing Taxi trip data with 'pickup_time' and 'trip_distance' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns 'month' and 'total_distance' representing \n",
    "                      the total distance traveled per month for Uber and Taxi combined.\n",
    "    \"\"\"\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2024-08-31\"\n",
    "    \n",
    "    # Combine both datasets into one dataframe\n",
    "    combined_data = pd.concat(\n",
    "        [\n",
    "            uber_data[[\"pickup_time\", \"trip_distance\"]].assign(type=\"Uber\"),\n",
    "            taxi_data[[\"pickup_time\", \"trip_distance\"]].assign(type=\"Taxi\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Filter data by date range\n",
    "    combined_data = combined_data[\n",
    "        (combined_data[\"pickup_time\"] >= start_date) & \n",
    "        (combined_data[\"pickup_time\"] <= end_date)\n",
    "    ]\n",
    "    \n",
    "    # Extract month\n",
    "    combined_data[\"month\"] = combined_data[\"pickup_time\"].dt.month\n",
    "\n",
    "    # Group by month and calculate total distance\n",
    "    grouped_data = combined_data.groupby(\"month\")[\"trip_distance\"].sum().reset_index()\n",
    "    grouped_data.rename(columns={\"trip_distance\": \"total_distance\"}, inplace=True)\n",
    "\n",
    "    return grouped_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc10d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total distance traveled per month for Uber and Taxi combined\n",
    "def plot_monthly_total_distance(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the total distance traveled per month for Uber and Taxi combined.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): A DataFrame containing:\n",
    "            - 'month': Month of the year (1-12).\n",
    "            - 'total_distance': Total distance traveled in that month.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=dataframe,\n",
    "        x=\"month\",\n",
    "        y=\"total_distance\",\n",
    "        marker=\"o\",\n",
    "        errorbar=None\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Total Distance Traveled per Month (2020-2024)\", fontsize=16)\n",
    "    plt.xlabel(\"Month\", fontsize=12)\n",
    "    plt.ylabel(\"Total Distance (miles)\", fontsize=12)\n",
    "    plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b84a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_combined_data = get_combined_monthly_distance_data(uber_data, taxi_data)\n",
    "plot_monthly_total_distance(v2_combined_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f4b7",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the third visualization\n",
    "airport_bboxes = {\n",
    "    \"LGA\": {\"lat_min\": 40.766, \"lat_max\": 40.793, \"lon_min\": -73.896, \"lon_max\": -73.863},\n",
    "    \"JFK\": {\"lat_min\": 40.641, \"lat_max\": 40.665, \"lon_min\": -73.79, \"lon_max\": -73.75},\n",
    "    \"EWR\": {\"lat_min\": 40.683, \"lat_max\": 40.7, \"lon_min\": -74.2, \"lon_max\": -74.15},\n",
    "}\n",
    "\n",
    "def filter_rides_to_airport(\n",
    "    df: pd.DataFrame, airport_name: str, bbox: Dict[str, float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter rides to a specific airport based on bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing ride data with 'dropoff_latitude' and 'dropoff_longitude' columns.\n",
    "        airport_name (str): Name of the airport (e.g., \"LGA\", \"JFK\", \"EWR\").\n",
    "        bbox (Dict[str, float]): Bounding box dictionary with lat_min, lat_max, lon_min, and lon_max.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame of rides to the specified airport, with an 'airport' column added.\n",
    "    \"\"\"\n",
    "    return df[\n",
    "        (df[\"dropoff_latitude\"] >= bbox[\"lat_min\"]) &\n",
    "        (df[\"dropoff_latitude\"] <= bbox[\"lat_max\"]) &\n",
    "        (df[\"dropoff_longitude\"] >= bbox[\"lon_min\"]) &\n",
    "        (df[\"dropoff_longitude\"] <= bbox[\"lon_max\"])\n",
    "    ].assign(airport=airport_name)\n",
    "\n",
    "def prepare_weekly_airport_ride_data(\n",
    "    uber_data: pd.DataFrame, taxi_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare data for visualizing the number of rides to each airport per day of the week.\n",
    "\n",
    "    Args:\n",
    "        uber_data (pd.DataFrame): Uber ride data with 'dropoff_time', 'dropoff_latitude', and 'dropoff_longitude' columns.\n",
    "        taxi_data (pd.DataFrame): Taxi ride data with 'dropoff_time', 'dropoff_latitude', and 'dropoff_longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame grouped by 'airport' and 'day_of_week', with the number of rides as 'num_rides'.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for airport, bbox in airport_bboxes.items():\n",
    "        filtered_data.append(filter_rides_to_airport(uber_data, airport, bbox))\n",
    "        filtered_data.append(filter_rides_to_airport(taxi_data, airport, bbox))\n",
    "    \n",
    "\n",
    "    airport_data = pd.concat(filtered_data)\n",
    "    \n",
    "    airport_data[\"day_of_week\"] = airport_data[\"dropoff_time\"].dt.day_name()\n",
    "    \n",
    "    grouped_data = airport_data.groupby([\"airport\", \"day_of_week\"]).size().reset_index(name=\"num_rides\")\n",
    "    \n",
    "    day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    grouped_data[\"day_of_week\"] = pd.Categorical(grouped_data[\"day_of_week\"], categories=day_order, ordered=True)\n",
    "    \n",
    "    return grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of rides to NYC airports by day of the week\n",
    "def plot_rides_to_airports_by_weekday(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the number of rides to NYC airports by day of the week.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): A DataFrame containing:\n",
    "            - 'day_of_week': The day of the week (e.g., \"Monday\").\n",
    "            - 'num_rides': The number of rides to the airport.\n",
    "            - 'airport': The name of the airport (e.g., \"JFK\", \"LGA\", \"EWR\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.barplot(\n",
    "        data=dataframe,\n",
    "        x=\"day_of_week\",\n",
    "        y=\"num_rides\",\n",
    "        hue=\"airport\",\n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    plt.title(\"Most Popular Day of the Week for Rides to NYC Airports\", fontsize=16)\n",
    "    plt.xlabel(\"Day of Week\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Rides\", fontsize=12)\n",
    "    plt.legend(title=\"Airport\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_3 = prepare_weekly_airport_ride_data(uber_data, taxi_data)\n",
    "plot_rides_to_airports_by_weekday(v_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14caa8c",
   "metadata": {},
   "source": [
    "### Visualization 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_monthly_earnings_data(taxi_data: pd.DataFrame, uber_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare data for monthly earnings visualization from taxi and Uber datasets.\n",
    "\n",
    "    Args:\n",
    "        taxi_data (pd.DataFrame): Taxi trip data with 'pickup_time' and fare-related columns.\n",
    "        uber_data (pd.DataFrame): Uber trip data with 'pickup_time' and fare-related columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame with monthly earnings breakdown for Taxi and Uber rides.\n",
    "    \"\"\"\n",
    "    # Extract month for grouping\n",
    "    taxi_data[\"month\"] = taxi_data[\"pickup_time\"].dt.to_period(\"M\")\n",
    "    uber_data[\"month\"] = uber_data[\"pickup_time\"].dt.to_period(\"M\")\n",
    "\n",
    "    # Process Taxi Data\n",
    "    taxi_grouped = taxi_data.groupby(\"month\").agg(\n",
    "        total_fare=(\"fare_amount\", \"sum\"),\n",
    "        base_fare=(\"fare_amount\", \"sum\"),\n",
    "        surcharges=(\"mta_tax\", \"sum\"),\n",
    "        tolls=(\"tolls_amount\", \"sum\"),\n",
    "        taxes=(\"tip_amount\", \"sum\")\n",
    "    ).reset_index()\n",
    "    taxi_grouped[\"ride_type\"] = \"Taxi\"\n",
    "\n",
    "    # Process Uber Data\n",
    "    uber_grouped = uber_data.groupby(\"month\").agg(\n",
    "        total_fare=(\"base_passenger_fare\", \"sum\"),\n",
    "        base_fare=(\"base_passenger_fare\", \"sum\"),\n",
    "        surcharges=(\"congestion_surcharge\", \"sum\"),\n",
    "        tolls=(\"tolls\", \"sum\"),\n",
    "        taxes=(\"sales_tax\", \"sum\")\n",
    "    ).reset_index()\n",
    "    uber_grouped[\"ride_type\"] = \"Uber\"\n",
    "\n",
    "    # Combine Taxi and Uber Data\n",
    "    combined_data = pd.concat([taxi_grouped, uber_grouped], ignore_index=True)\n",
    "\n",
    "    # Ensure month is in datetime format for sorting\n",
    "    combined_data[\"month\"] = combined_data[\"month\"].dt.to_timestamp()\n",
    "\n",
    "    return combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_earnings(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the monthly earnings breakdown for taxis and Ubers.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): A DataFrame containing:\n",
    "            - 'month': The month of the earnings data (datetime format).\n",
    "            - 'total_fare': Total earnings for the month.\n",
    "            - 'ride_type': Type of ride ('Taxi' or 'Uber').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(\n",
    "        data=dataframe,\n",
    "        x=\"month\",\n",
    "        y=\"total_fare\",\n",
    "        hue=\"ride_type\",\n",
    "        marker=\"o\"\n",
    "    )\n",
    "    plt.title(\"Monthly Total Earnings by Ride Type (2020-2024)\", fontsize=16)\n",
    "    plt.xlabel(\"Month\", fontsize=12)\n",
    "    plt.ylabel(\"Total Earnings ($)\", fontsize=12)\n",
    "    plt.legend(title=\"Ride Type\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdad35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = prepare_monthly_earnings_data(taxi_data, uber_data)\n",
    "plot_monthly_earnings(combined_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285447fc",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the fifth visualization\n",
    "def plot_tips_vs_factors_hourly(\n",
    "    uber_data: pd.DataFrame, \n",
    "    taxi_data: pd.DataFrame, \n",
    "    hourly_weather_data: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the relationship between tips, distance, and precipitation for Uber and Yellow Taxi rides.\n",
    "\n",
    "    Args:\n",
    "        uber_data (pd.DataFrame): Uber trip data containing 'pickup_time', 'trip_distance', and 'tips'.\n",
    "        taxi_data (pd.DataFrame): Taxi trip data containing 'pickup_time', 'trip_distance', and 'tips'.\n",
    "        hourly_weather_data (pd.DataFrame): Weather data containing 'hourly_time' and 'hourly_precipitation'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if \"tips\" not in taxi_data.columns:\n",
    "        taxi_data = taxi_data.rename(columns={\"tip_amount\": \"tips\"})\n",
    "    \n",
    "    start_date = \"2022-01-01\"\n",
    "    end_date = \"2023-12-31\"\n",
    "    \n",
    "    uber_filtered = uber_data[(uber_data[\"pickup_time\"] >= start_date) & (uber_data[\"pickup_time\"] <= end_date)]\n",
    "    taxi_filtered = taxi_data[(taxi_data[\"pickup_time\"] >= start_date) & (taxi_data[\"pickup_time\"] <= end_date)]\n",
    "    \n",
    "    hourly_weather_data[\"hourly_time\"] = pd.to_datetime(hourly_weather_data[\"hourly_time\"])\n",
    "    taxi_filtered[\"hourly_time\"] = taxi_filtered[\"pickup_time\"].dt.floor(\"H\")\n",
    "    uber_filtered[\"hourly_time\"] = uber_filtered[\"pickup_time\"].dt.floor(\"H\")\n",
    "    \n",
    "    taxi_filtered = pd.merge(\n",
    "        taxi_filtered,\n",
    "        hourly_weather_data,\n",
    "        how=\"left\",\n",
    "        left_on=\"hourly_time\",\n",
    "        right_on=\"hourly_time\"\n",
    "    )\n",
    "    uber_filtered = pd.merge(\n",
    "        uber_filtered,\n",
    "        hourly_weather_data,\n",
    "        how=\"left\",\n",
    "        left_on=\"hourly_time\",\n",
    "        right_on=\"hourly_time\"\n",
    "    )\n",
    "    \n",
    "    # Remove unusual tips (e.g., tips over $100)\n",
    "    taxi_filtered = taxi_filtered[taxi_filtered[\"tips\"] < 100]\n",
    "    uber_filtered = uber_filtered[uber_filtered[\"tips\"] < 100]\n",
    "    \n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(\"Effect of Distance and Precipitation on Tips\", fontsize=16)\n",
    "\n",
    "    # 1Yellow Taxi - Tips vs Distance\n",
    "    sns.scatterplot(\n",
    "        data=taxi_filtered,\n",
    "        x=\"trip_distance\",\n",
    "        y=\"tips\",\n",
    "        ax=axes[0, 0],\n",
    "        color=\"blue\",\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[0, 0].set_title(\"Yellow Taxi: Tips vs Distance\")\n",
    "    axes[0, 0].set_xlabel(\"Trip Distance (miles)\")\n",
    "    axes[0, 0].set_ylabel(\"Tips (USD)\")\n",
    "\n",
    "    # 2Uber - Tips vs Distance\n",
    "    sns.scatterplot(\n",
    "        data=uber_filtered,\n",
    "        x=\"trip_distance\",\n",
    "        y=\"tips\",\n",
    "        ax=axes[0, 1],\n",
    "        color=\"green\",\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[0, 1].set_title(\"Uber: Tips vs Distance\")\n",
    "    axes[0, 1].set_xlabel(\"Trip Distance (miles)\")\n",
    "    axes[0, 1].set_ylabel(\"Tips (USD)\")\n",
    "\n",
    "    # 3Yellow Taxi - Tips vs Precipitation\n",
    "    sns.scatterplot(\n",
    "        data=taxi_filtered,\n",
    "        x=\"hourly_precipitation\",\n",
    "        y=\"tips\",\n",
    "        ax=axes[1, 0],\n",
    "        color=\"purple\",\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Yellow Taxi: Tips vs Precipitation\")\n",
    "    axes[1, 0].set_xlabel(\"Precipitation (inches)\")\n",
    "    axes[1, 0].set_ylabel(\"Tips (USD)\")\n",
    "\n",
    "    # 4Uber - Tips vs Precipitation\n",
    "    sns.scatterplot(\n",
    "        data=uber_filtered,\n",
    "        x=\"hourly_precipitation\",\n",
    "        y=\"tips\",\n",
    "        ax=axes[1, 1],\n",
    "        color=\"orange\",\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Uber: Tips vs Precipitation\")\n",
    "    axes[1, 1].set_xlabel(\"Precipitation (inches)\")\n",
    "    axes[1, 1].set_ylabel(\"Tips (USD)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "plot_tips_vs_factors_hourly(uber_data, taxi_data, hourly_weather_data)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849a4c7",
   "metadata": {},
   "source": [
    "### Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_time to datetime in both datasets\n",
    "uber_data['pickup_time'] = pd.to_datetime(uber_data['pickup_time'])\n",
    "taxi_data['pickup_time'] = pd.to_datetime(taxi_data['pickup_time'])\n",
    "\n",
    "# Filter for trips in 2020\n",
    "uber_2020 = uber_data[uber_data['pickup_time'].dt.year == 2020]\n",
    "taxi_2020 = taxi_data[taxi_data['pickup_time'].dt.year == 2020]\n",
    "\n",
    "# Add a column to represent trip counts (1 per trip)\n",
    "uber_2020['trips'] = 1\n",
    "taxi_2020['trips'] = 1\n",
    "\n",
    "# Combine Uber and Taxi data with relevant columns\n",
    "combined_data = pd.concat([\n",
    "    uber_2020[['pickup_latitude', 'pickup_longitude', 'trips']],\n",
    "    taxi_2020[['pickup_latitude', 'pickup_longitude', 'trips']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN values in latitude or longitude\n",
    "combined_data = combined_data.dropna(subset=['pickup_latitude', 'pickup_longitude'])\n",
    "\n",
    "# Aggregate trip counts by unique pickup coordinates\n",
    "heatmap_data = combined_data.groupby(['pickup_latitude', 'pickup_longitude'], as_index=False)['trips'].sum()\n",
    "\n",
    "# Handle NaNs: Ensure no NaN values in latitude or longitude\n",
    "heatmap_data = heatmap_data.dropna(subset=['pickup_latitude', 'pickup_longitude'])\n",
    "\n",
    "# Prepare heatmap data as a list of [latitude, longitude, weight]\n",
    "heatmap_data_list = heatmap_data[['pickup_latitude', 'pickup_longitude', 'trips']].values.tolist()\n",
    "\n",
    "# Calculate map center (mean latitude and longitude), ignoring NaNs\n",
    "map_center = [\n",
    "    heatmap_data['pickup_latitude'].mean(skipna=True), \n",
    "    heatmap_data['pickup_longitude'].mean(skipna=True)\n",
    "]\n",
    "\n",
    "# Ensure map center has valid coordinates\n",
    "if pd.isna(map_center[0]) or pd.isna(map_center[1]):\n",
    "    print(\"Map center has invalid coordinates due to insufficient data.\")\n",
    "else:\n",
    "    # Create a Folium map centered at the calculated mean coordinates\n",
    "    heatmap_map = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "    # Add a heatmap layer to the map\n",
    "    HeatMap(heatmap_data_list, radius=10).add_to(heatmap_map)\n",
    "\n",
    "display(heatmap_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941195a",
   "metadata": {},
   "source": [
    "### Extra credit 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_monthly_distance(dataframe: pd.DataFrame) -> HTML:\n",
    "    \"\"\"\n",
    "    Create an animated line and scatter plot to visualize average distance traveled per month.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): A DataFrame containing:\n",
    "            - 'month': Month of the year (1-12).\n",
    "            - 'total_distance': Total distance traveled for each month.\n",
    "\n",
    "    Returns:\n",
    "        HTML: An animation object displaying the visualization.\n",
    "    \"\"\"\n",
    "    # Extract months and total distances\n",
    "    months = dataframe[\"month\"]\n",
    "    distances = dataframe[\"total_distance\"]\n",
    "\n",
    "    # Set up the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    line, = ax.plot([], [], marker=\"o\", color=\"blue\", label=\"Total Distance Traveled\")\n",
    "    scatter = ax.scatter([], [], color=\"blue\", s=50)\n",
    "\n",
    "    # Set up plot aesthetics\n",
    "    ax.set_xlim(1, 12)\n",
    "    ax.set_ylim(0, distances.max() + 1000)\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels([\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "    ax.set_title(\"Average Distance Traveled per Month (2020-2024)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Month\", fontsize=12)\n",
    "    ax.set_ylabel(\"Average Distance (miles)\", fontsize=12)\n",
    "    ax.legend()\n",
    "\n",
    "    # Update function for animation\n",
    "    def update(frame):\n",
    "        line.set_data(months[:frame], distances[:frame])\n",
    "        scatter.set_offsets(np.column_stack((months[:frame], distances[:frame])))\n",
    "        return line, scatter\n",
    "\n",
    "    # Create the animation\n",
    "    ani = FuncAnimation(fig, update, frames=len(months), blit=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "# Display\n",
    "v2_combined_data = get_combined_monthly_distance_data(uber_data, taxi_data)\n",
    "animate_monthly_distance(v2_combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec5c44",
   "metadata": {},
   "source": [
    "##### Extra Credit 4: What is the total number of hired rides within one hour before sunrise and sunset between 2023-12-15 and 2023-12-25?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_7_FILENAME = \"query7.sql\"\n",
    "\n",
    "QUERY_7 = \"\"\"\n",
    "WITH daily_sunrise_sunset AS (\n",
    "    SELECT\n",
    "        DATE(date) AS ride_date,\n",
    "        TIME(sunrise) AS sunrise_time,\n",
    "        TIME(sunset) AS sunset_time\n",
    "    FROM\n",
    "        daily_weather\n",
    "    WHERE\n",
    "        date BETWEEN '2023-12-15' AND '2023-12-25'\n",
    "),\n",
    "sunrise_sunset_periods AS (\n",
    "    SELECT\n",
    "        ride_date,\n",
    "        datetime(ride_date || ' ' || sunrise_time, '-1 hour') AS sunrise_start,\n",
    "        datetime(ride_date || ' ' || sunrise_time, '+1 hour') AS sunrise_end,\n",
    "        datetime(ride_date || ' ' || sunset_time, '-1 hour') AS sunset_start,\n",
    "        datetime(ride_date || ' ' || sunset_time, '+1 hour') AS sunset_end\n",
    "    FROM\n",
    "        daily_sunrise_sunset\n",
    "),\n",
    "rides_sunrise_sunset AS (\n",
    "    SELECT\n",
    "        sp.ride_date,\n",
    "        COUNT(CASE WHEN u.pickup_time BETWEEN sp.sunrise_start AND sp.sunrise_end THEN 1 END) +\n",
    "        COUNT(CASE WHEN t.pickup_time BETWEEN sp.sunrise_start AND sp.sunrise_end THEN 1 END) AS total_rides_sunrise,\n",
    "        COUNT(CASE WHEN u.pickup_time BETWEEN sp.sunset_start AND sp.sunset_end THEN 1 END) +\n",
    "        COUNT(CASE WHEN t.pickup_time BETWEEN sp.sunset_start AND sp.sunset_end THEN 1 END) AS total_rides_sunset\n",
    "    FROM\n",
    "        sunrise_sunset_periods sp\n",
    "    LEFT JOIN\n",
    "        uber_trips u\n",
    "    ON\n",
    "        DATE(u.pickup_time) = sp.ride_date\n",
    "    LEFT JOIN\n",
    "        taxi_trips t\n",
    "    ON\n",
    "        DATE(t.pickup_time) = sp.ride_date\n",
    "    WHERE\n",
    "        (u.pickup_time BETWEEN sp.sunrise_start AND sp.sunrise_end\n",
    "         OR u.pickup_time BETWEEN sp.sunset_start AND sp.sunset_end)\n",
    "        OR\n",
    "        (t.pickup_time BETWEEN sp.sunrise_start AND sp.sunrise_end\n",
    "         OR t.pickup_time BETWEEN sp.sunset_start AND sp.sunset_end)\n",
    "    GROUP BY\n",
    "        sp.ride_date\n",
    ")\n",
    "SELECT\n",
    "    ride_date,\n",
    "    total_rides_sunrise,\n",
    "    total_rides_sunset\n",
    "FROM\n",
    "    rides_sunrise_sunset\n",
    "ORDER BY\n",
    "    ride_date ASC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_7)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_7, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "data = pd.read_sql(QUERY_7, con=engine)\n",
    "\n",
    "\n",
    "# Convert ride_date to datetime for better plotting\n",
    "data['ride_date'] = pd.to_datetime(data['ride_date'])\n",
    "\n",
    "# Plot settings\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.4\n",
    "\n",
    "# X-axis positions for bars\n",
    "x = range(len(data))\n",
    "\n",
    "# Plot Sunrise and Sunset rides as side-by-side bars\n",
    "bars_sunrise = ax.bar(x, data['total_rides_sunrise'], width=bar_width, label='Sunrise Rides', color='skyblue', alpha=0.8)\n",
    "bars_sunset = ax.bar([p + bar_width for p in x], data['total_rides_sunset'], width=bar_width, label='Sunset Rides', color='orange', alpha=0.8)\n",
    "\n",
    "# X-axis labels\n",
    "ax.set_xticks([p + bar_width / 2 for p in x])\n",
    "ax.set_xticklabels(data['ride_date'].dt.strftime('%Y-%m-%d'), rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Add labels on top of bars\n",
    "for bar in bars_sunrise:\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        str(bar.get_height()),\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "for bar in bars_sunset:\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        str(bar.get_height()),\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Total Rides', fontsize=12)\n",
    "ax.set_title('Total Rides During Sunrise and Sunset', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Grid for better readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4baf2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
